{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Large Language Models \u2014 Lab Notebook\n",
        "Applied Data Science (CPSC 8xxx) \u00b7 Clemson University\n",
        "Instructor: _[Your Name]_\n",
        "Date: 2025-11-02\n",
        "\n",
        "This notebook accompanies the 75-minute lecture on multimodal large language models. It is designed to run on the Clemson Palmetto cluster and demonstrates how to stage data, configure training jobs, and fine-tune state-of-the-art multimodal foundation models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Outcomes\n",
        "By working through this lab you will:\n",
        "\n",
        "- Stand up a reproducible multimodal training environment on Palmetto.\n",
        "- Implement data pipelines that combine text, vision, and audio modalities.\n",
        "- Train a compact multimodal encoder-decoder model from scratch on a curated dataset.\n",
        "- Fine-tune open-source any-to-any assistants using parameter-efficient techniques.\n",
        "- Evaluate, profile, and monitor multimodal workloads responsibly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Prerequisites\n",
        "- Palmetto account with access to GPU partitions (preferably `gpu-a100` or `gpu-v100`).\n",
        "- An interactive or batch allocation via SLURM.\n",
        "- Conda (Miniconda or Mamba) installed under your home directory.\n",
        "- Sufficient quota on `/scratch1` for temporary datasets (~200 GB recommended).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Session Outline\n",
        "1. Environment bootstrap on Palmetto\n",
        "2. Data staging & multimodal manifest creation\n",
        "3. Training a compact vision-language model from scratch\n",
        "4. Fine-tuning LLaVA-style instruction followers\n",
        "5. Extending to audio-text and video-text adapters\n",
        "6. Evaluation, monitoring, and safety audits\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Check GPU availability (run inside an interactive GPU session)\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Environment Bootstrap\n",
        "Use a dedicated Conda environment per experiment to keep dependencies isolated. The snippet below assumes the CUDA 12 toolchain on Palmetto. Adjust the module versions as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "module purge\n",
        "module load gcc/11.3.0 cuda/12.1.0 cudnn/8.9.2.26\n",
        "source $HOME/miniconda3/etc/profile.d/conda.sh\n",
        "conda create -y -n multimodal-llm python=3.10\n",
        "conda activate multimodal-llm\n",
        "pip install --upgrade pip\n",
        "pip install 'torch==2.1.*' --index-url https://download.pytorch.org/whl/cu121\n",
        "pip install accelerate transformers datasets einops timm bitsandbytes peft open_clip_torch torchvision torchaudio\n",
        "pip install soundfile decord webdataset pillow scipy wandb mlflow rich\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Tip:** For repeated jobs, bake the environment into a Palmetto Singularity container or use Conda-pack to speed up startup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Staging & Manifest Creation\n",
        "For this lab we will sample a small but diverse multimodal corpus:\n",
        "- Image-text pairs: subset of [LAION-400M](https://laion.ai/blog/laion-400-open-dataset/)\n",
        "- Instructional dialogues: LLaVA-Instruct 150K\n",
        "- Audio-caption pairs: AudioCaps + synthetic prompts\n",
        "\n",
        "Stage the raw archives onto `/scratch1/$USER/datasets/multimodal` using `rclone`, `aws s3 cp`, or the Clemson Globus endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "data_root = Path('/scratch1') / os.environ.get('USER', 'student') / 'datasets' / 'multimodal'\n",
        "print('Dataset root:', data_root)\n",
        "\n",
        "data_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "image_text_manifest = data_root / 'laion_subset.tsv'\n",
        "if not image_text_manifest.exists():\n",
        "    with image_text_manifest.open('w') as f:\n",
        "        f.write('url\tcaption\n",
        "')\n",
        "        f.write('https://example.org/image1.jpg\tA robot assembling solar panels on campus.\n",
        "')\n",
        "        f.write('https://example.org/image2.jpg\tStudents collaborating with an interactive whiteboard.\n",
        "')\n",
        "print('Created manifest:', image_text_manifest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download helpers\n",
        "Use `multiprocessing` or SLURM array jobs to fetch shards in parallel. The following utility script expects a TSV manifest and stores files under `/scratch1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import concurrent.futures as futures\n",
        "import requests\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers['User-Agent'] = 'Clemson-Multimodal-Lab/1.0'\n",
        "\n",
        "def download_example(row):\n",
        "    url, caption = row\n",
        "    target = data_root / 'images' / Path(url).name\n",
        "    target.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if target.exists():\n",
        "        return target\n",
        "    try:\n",
        "        resp = session.get(url, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        target.write_bytes(resp.content)\n",
        "        return target\n",
        "    except Exception as exc:\n",
        "        print('Failed to fetch', url, exc)\n",
        "        return None\n",
        "\n",
        "rows = [('https://example.org/image1.jpg', 'A robot assembling solar panels on campus.')]\n",
        "with futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for result in executor.map(download_example, rows):\n",
        "        print('Downloaded ->', result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Warm-Up: Zero-Shot CLIP Retrieval\n",
        "Before training from scratch, verify the environment by running inference with OpenCLIP.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device='cuda')\n",
        "tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
        "\n",
        "candidate_urls = [\n",
        "    'https://images.unsplash.com/photo-1522199994827-8f68f1d1d8c5',\n",
        "    'https://images.unsplash.com/photo-1581092152835-30ab079f19b9',\n",
        "]\n",
        "images = []\n",
        "for url in candidate_urls:\n",
        "    img = Image.open(BytesIO(requests.get(url).content)).convert('RGB')\n",
        "    images.append(preprocess(img))\n",
        "image_tensor = torch.stack(images).cuda()\n",
        "texts = tokenizer([\n",
        "    'Students collaborating with augmented reality interfaces',\n",
        "    'Industrial robot arm assembling circuit boards'\n",
        "])\n",
        "text_tensor = texts.cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_tensor)\n",
        "    text_features = model.encode_text(text_tensor)\n",
        "    logits = (image_features @ text_features.T) / model.logit_scale.exp()\n",
        "print('Similarity matrix:', logits.softmax(dim=-1).cpu())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Training a Compact Vision-Language Model from Scratch\n",
        "We construct a dual-encoder similar to CLIP but sized for a classroom-scale dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    vision_model: str = 'openai/clip-vit-base-patch16'\n",
        "    text_model: str = 'distilbert-base-uncased'\n",
        "    batch_size: int = 256\n",
        "    lr: float = 5e-4\n",
        "    warmup_steps: int = 500\n",
        "    total_steps: int = 5000\n",
        "    log_every: int = 50\n",
        "    output_dir: str = f\"/scratch1/{os.environ.get('USER', 'student')}/experiments/clip-scratch\"\n",
        "    bf16: bool = True\n",
        "\n",
        "config = TrainingConfig()\n",
        "print(config)\n",
        "\n",
        "dataset = load_dataset('laion/laion400m', split='train[:0.02%]', streaming=True)\n",
        "\n",
        "vision_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "vision_encoder = AutoModel.from_pretrained(config.vision_model)\n",
        "text_encoder = AutoModel.from_pretrained(config.text_model)\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(config.text_model)\n",
        "\n",
        "vision_encoder.train()\n",
        "text_encoder.train()\n",
        "\n",
        "projector = nn.Linear(vision_encoder.config.hidden_size, text_encoder.config.hidden_size)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    vision_encoder.cuda()\n",
        "    text_encoder.cuda()\n",
        "    projector.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import itertools\n",
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(mixed_precision='bf16' if config.bf16 else 'no')\n",
        "\n",
        "params = itertools.chain(vision_encoder.parameters(), text_encoder.parameters(), projector.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=config.lr, weight_decay=0.01)\n",
        "\n",
        "temperature = torch.nn.Parameter(torch.tensor(0.07))\n",
        "if torch.cuda.is_available():\n",
        "    temperature = temperature.to(accelerator.device)\n",
        "\n",
        "vision_encoder, text_encoder, projector, optimizer, temperature = accelerator.prepare(\n",
        "    vision_encoder, text_encoder, projector, optimizer, temperature\n",
        ")\n",
        "\n",
        "step = 0\n",
        "for batch in dataset.take(config.total_steps):\n",
        "    images = batch['image']\n",
        "    captions = batch['caption']\n",
        "    pixel_values = torch.stack([vision_transform(img) for img in images])\n",
        "    tokenized = text_tokenizer(list(captions), return_tensors='pt', padding=True, truncation=True)\n",
        "    pixel_values = pixel_values.to(accelerator.device)\n",
        "    tokenized = {k: v.to(accelerator.device) for k, v in tokenized.items()}\n",
        "\n",
        "    image_feats = vision_encoder(pixel_values=pixel_values).pooler_output\n",
        "    text_feats = text_encoder(**tokenized).last_hidden_state[:, 0]\n",
        "    image_feats = projector(image_feats)\n",
        "\n",
        "    image_feats = F.normalize(image_feats, dim=-1)\n",
        "    text_feats = F.normalize(text_feats, dim=-1)\n",
        "    logits = (image_feats @ text_feats.t()) / temperature.exp()\n",
        "    targets = torch.arange(logits.size(0), device=logits.device)\n",
        "    loss = (F.cross_entropy(logits, targets) + F.cross_entropy(logits.t(), targets)) / 2\n",
        "\n",
        "    accelerator.backward(loss)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if step % config.log_every == 0:\n",
        "        accelerator.print(f\"step={step} loss={loss.item():.4f}\")\n",
        "    step += 1\n",
        "    if step >= config.total_steps:\n",
        "        break\n",
        "\n",
        "accelerator.print('Training completed')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Checkpointing & Evaluation\n",
        "Persist checkpoints to `/scratch1/$USER/experiments` and periodically evaluate on COCO or Flickr30k. Use Weights & Biases or MLflow for tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "save_dir = Path(config.output_dir)\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "unwrap_vision = accelerator.unwrap_model(vision_encoder)\n",
        "unwrap_text = accelerator.unwrap_model(text_encoder)\n",
        "\n",
        "accelerator.save_state(save_dir / 'accelerate_state')\n",
        "unwrap_vision.save_pretrained(save_dir / 'vision_encoder')\n",
        "unwrap_text.save_pretrained(save_dir / 'text_encoder')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Visual Instruction Fine-Tuning (LLaVA-1.5 style)\n",
        "We reuse the pretrained CLIP vision tower, attach a projection MLP, and supervise with multimodal conversation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained('liuhaotian/llava-v1.5-7b', device_map='auto', torch_dtype=torch.bfloat16)\n",
        "vision_projector = nn.Sequential(\n",
        "    nn.Linear(vision_encoder.config.hidden_size, llm.config.hidden_size),\n",
        "    nn.GELU(),\n",
        "    nn.Linear(llm.config.hidden_size, llm.config.hidden_size)\n",
        ")\n",
        "vision_projector = get_peft_model(vision_projector, LoraConfig(r=8, lora_alpha=16, target_modules=['0', '2']))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "batch = {\n",
        "    'pixel_values': torch.randn(2, 3, 224, 224, device=llm.device),\n",
        "    'input_ids': torch.ones((2, 128), dtype=torch.long, device=llm.device),\n",
        "    'attention_mask': torch.ones((2, 128), dtype=torch.long, device=llm.device),\n",
        "    'labels': torch.ones((2, 128), dtype=torch.long, device=llm.device),\n",
        "}\n",
        "outputs = llm(**batch)\n",
        "loss = outputs.loss\n",
        "loss.backward()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Audio-Text Alignment Adapter\n",
        "We extend the framework to speech prompts using Whisper encoder features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperModel\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained('openai/whisper-small')\n",
        "whisper = WhisperModel.from_pretrained('openai/whisper-small')\n",
        "\n",
        "waveform = torch.randn(1, 16000 * 10)\n",
        "inputs = processor(waveform, sampling_rate=16000, return_tensors='pt')\n",
        "audio_features = whisper.encoder(inputs.input_features).last_hidden_state.mean(dim=1)\n",
        "audio_to_text = nn.Linear(audio_features.size(-1), llm.config.hidden_size)\n",
        "audio_embeddings = audio_to_text(audio_features)\n",
        "print(audio_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Video Token Resampler\n",
        "For video-text tasks, subsample frames and use a temporal adapter.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import decord\n",
        "from decord import VideoReader\n",
        "\n",
        "vr = VideoReader('sample.mp4', num_threads=1)\n",
        "frame_ids = list(range(0, len(vr), max(len(vr)//16, 1)))[:16]\n",
        "frames = vr.get_batch(frame_ids).asnumpy()\n",
        "frames_tensor = torch.from_numpy(frames).permute(0, 3, 1, 2) / 255.0\n",
        "\n",
        "temporal_pool = nn.Conv1d(in_channels=frames_tensor.shape[0], out_channels=8, kernel_size=1)\n",
        "video_tokens = temporal_pool(frames_tensor.view(frames_tensor.shape[0], -1, frames_tensor.shape[1]*frames_tensor.shape[2]*frames_tensor.shape[3]))\n",
        "print('Video tokens shape:', video_tokens.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Evaluation & Safety Audits\n",
        "- **Retrieval**: Recall@K on MSCOCO, Flickr30k.\n",
        "- **Instruction following**: MMBench, MMMU, ScienceQA.\n",
        "- **Audio/Text**: WER, BLEU, MOS.\n",
        "- **Bias/Safety**: Multimodal SafetyBench, RealToxicityPrompts with image perturbations.\n",
        "\n",
        "Log responsible AI metrics in the same experiment tracking dashboard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Homework Preview\n",
        "1. Fine-tune the CLIP-like model on a Clemson campus dataset.\n",
        "2. Adapt LLaVA to support chart understanding using the ChartQA dataset.\n",
        "3. Train an audio-text adapter for campus tour narration.\n",
        "\n",
        "Refer to the dedicated homework notebook for detailed instructions and grading rubrics.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}