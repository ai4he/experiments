#!/usr/bin/env python3
"""
COMPLETE COMPREHENSIVE PRESENTATION - ALL 78 SLIDES
Systematic addition of detailed theoretical content to every slide
3-5 paragraphs per slide with mathematical formulations
"""

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.enum.text import PP_ALIGN, MSO_ANCHOR
from pptx.dml.color import RGBColor
import sys
import io

# Import diagram functions from existing script
exec(open('/home/user/experiments/generate_full_presentation.py').read().split('def create_presentation')[0])

CLEMSON_ORANGE = RGBColor(246, 103, 51)
CLEMSON_PURPLE = RGBColor(82, 45, 128)
DARK_GRAY = RGBColor(51, 51, 51)

slide_count = 0

def log_status(msg):
    global slide_count
    slide_count += 1
    print(f"[Slide {slide_count:2d}/78] {msg}")
    sys.stdout.flush()

print("="*80)
print("COMPREHENSIVE CONTENT GENERATION FOR ALL 78 SLIDES")
print("Adding detailed theoretical paragraphs systematically")
print("="*80)
print()

# Create presentation
prs = Presentation()
prs.slide_width = Inches(10)
prs.slide_height = Inches(7.5)

def add_image(slide, img_stream, left, top, width):
    slide.shapes.add_picture(img_stream, left, top, width)

def add_title_slide(title, subtitle="", img=None):
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    tb = slide.shapes.add_textbox(Inches(0.5), Inches(2.5), Inches(9), Inches(1))
    tf = tb.text_frame
    tf.text = title
    p = tf.paragraphs[0]
    p.alignment = PP_ALIGN.CENTER
    p.font.size = Pt(40)
    p.font.bold = True
    p.font.color.rgb = CLEMSON_ORANGE
    if subtitle:
        stb = slide.shapes.add_textbox(Inches(0.5), Inches(3.7), Inches(9), Inches(0.8))
        stf = stb.text_frame
        stf.text = subtitle
        sp = stf.paragraphs[0]
        sp.alignment = PP_ALIGN.CENTER
        sp.font.size = Pt(22)
        sp.font.color.rgb = DARK_GRAY
    if img:
        add_image(slide, img, Inches(3), Inches(4.8), Inches(4))
    return slide

def add_comprehensive_slide(title, paragraphs, img=None):
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    tb = slide.shapes.add_textbox(Inches(0.4), Inches(0.25), Inches(9.2), Inches(0.6))
    tf = tb.text_frame
    tf.text = title
    p = tf.paragraphs[0]
    p.font.size = Pt(26)
    p.font.bold = True
    p.font.color.rgb = CLEMSON_PURPLE

    if img:
        text_left, text_width = Inches(0.4), Inches(5.2)
        img_left, img_top, img_width = Inches(5.9), Inches(1.1), Inches(3.8)
    else:
        text_left, text_width = Inches(0.5), Inches(9)

    ctb = slide.shapes.add_textbox(text_left, Inches(1), text_width, Inches(6.2))
    ctf = ctb.text_frame
    ctf.word_wrap = True
    ctf.vertical_anchor = MSO_ANCHOR.TOP

    for i, para_text in enumerate(paragraphs):
        if i > 0:
            ctf.add_paragraph()
        p = ctf.paragraphs[i]
        p.text = para_text
        p.font.size = Pt(11)
        p.space_before = Pt(9)
        p.space_after = Pt(6)
        p.font.color.rgb = DARK_GRAY
        p.line_spacing = 1.2

    if img:
        add_image(slide, img, img_left, img_top, img_width)

    return slide

def add_section_slide(title):
    slide = prs.slides.add_slide(prs.slide_layouts[6])
    bg = slide.background
    fill = bg.fill
    fill.solid()
    fill.fore_color.rgb = CLEMSON_PURPLE
    tb = slide.shapes.add_textbox(Inches(1), Inches(3), Inches(8), Inches(1.5))
    tf = tb.text_frame
    tf.text = title
    p = tf.paragraphs[0]
    p.alignment = PP_ALIGN.CENTER
    p.font.size = Pt(44)
    p.font.bold = True
    p.font.color.rgb = RGBColor(255, 255, 255)
    return slide

# ============================================================================
# SLIDES 1-13: Already created comprehensive content
# ============================================================================

# SLIDE 1: Title Slide
log_status("Title slide with comprehensive overview")
img = create_diagram_multimodal_learning()
add_title_slide("Multimodal Large Language Models",
                "Applied Data Science - Clemson University", img)

# SLIDE 2: Course Context
log_status("Course Context with detailed theoretical background")
img = create_simple_diagram("Course Flow", ["Transformers", "LLMs", "Multimodal\nLLMs", "Applications"])
add_comprehensive_slide("Course Context", [
    "This lecture is positioned within the broader Applied Data Science curriculum after you have completed foundational courses on deep learning, Transformer architectures, and large language models. The prerequisite knowledge includes understanding attention mechanisms at a deep level—specifically self-attention (where each token attends to all other tokens in a sequence), cross-attention (where one sequence attends to another sequence), multi-head attention (parallel attention computations with different learned projections), and the overall Transformer encoder-decoder architecture introduced in Vaswani et al.'s seminal 'Attention Is All You Need' paper.",

    "In previous lessons, you studied single-modality large language models such as BERT (Bidirectional Encoder Representations from Transformers) for understanding tasks, GPT (Generative Pre-trained Transformer) for generation tasks, and T5 (Text-to-Text Transfer Transformer) for unified text processing. These models demonstrated remarkable capabilities when pre-trained on massive text corpora and then fine-tuned for specific tasks. However, they remained fundamentally limited to processing and generating text, unable to perceive visual information, understand audio, or ground language in perceptual experience.",

    "The transition to multimodal models represents a paradigm shift from isolated modality processing to integrated multi-sensory understanding. Just as humans don't process visual and linguistic information in complete isolation but rather build unified conceptual representations informed by both seeing and reading about objects and events, multimodal AI systems aim to learn joint representations that capture the relationships and correspondences across different data types. This integration enables entirely new capabilities: answering questions about images, generating images from text descriptions, understanding video narratives, and building embodied AI systems for robotics.",

    "Today's lecture will provide you with both theoretical foundations and practical implementation skills. You'll learn the mathematical principles underlying multimodal representation learning, study state-of-the-art architectures (CLIP, BLIP-2, Flamingo, LLaVA, GPT-4V), understand training strategies from scratch versus fine-tuning approaches, and gain hands-on experience implementing these models on Clemson's Palmetto HPC cluster. This prepares you for the accompanying lab session where you'll work with real multimodal datasets and the homework assignments where you'll fine-tune models for different tasks."
], img)

# SLIDE 3: Learning Objectives
log_status("Learning Objectives with comprehensive explanations")
img = create_simple_diagram("Learning Path", ["Theory", "Architectures", "Training", "Practice"])
add_comprehensive_slide("Learning Objectives", [
    "Theoretical Foundations and Mathematical Framework: You will develop a deep understanding of the theoretical principles underlying multimodal learning. This includes the mathematical formalization of multimodal representation learning, where we consider data from M different modalities X = {X₁, X₂, ..., Xₘ}, each potentially having different dimensionality dᵢ, structure, and statistical properties. You'll learn how to map these heterogeneous inputs into a common representation space Z ∈ ℝᵈ through learned projection functions. We'll study contrastive learning objectives in detail, particularly the InfoNCE (Noise Contrastive Estimation) loss: ℒ = -log[exp(sim(z_i, z_j)/τ) / Σₖ exp(sim(z_i, z_k)/τ)], where sim(·,·) is typically cosine similarity and τ is a learned temperature parameter.",

    "Architectural Mastery Across Multiple Paradigms: You will gain comprehensive knowledge of state-of-the-art multimodal architectures, understanding not just how they work but why they were designed as they are. For CLIP, you'll study the dual-encoder design where separate vision and text encoders project inputs into a shared d-dimensional space, trained with symmetric contrastive loss on hundreds of millions of image-text pairs. For BLIP-2, you'll understand the elegant Q-Former architecture that acts as a lightweight bridge between frozen pre-trained vision and language models, using learnable query tokens that extract visual features relevant for language tasks through cross-attention.",

    "Training Strategies from Foundations to Fine-tuning: You will master both training from scratch and fine-tuning methodologies. For training from scratch, you'll learn about data requirements (modern models train on 400 million to 5 billion image-text pairs), computational budgets (CLIP used 592 V100 GPUs for 12 days), pre-training objectives beyond contrastive learning (masked language modeling, masked image modeling, image-text matching), and optimization strategies. For fine-tuning, you'll study the spectrum from full fine-tuning to parameter-efficient fine-tuning (PEFT). You'll learn LoRA (Low-Rank Adaptation) in detail: instead of updating weight matrix W ∈ ℝᵈˣᵈ, we learn ΔW = BA where B ∈ ℝᵈˣʳ and A ∈ ℝʳˣᵈ with rank r << d.",

    "Practical Implementation and Deployment: You will gain hands-on expertise in implementing multimodal models using modern tools and infrastructure. This includes mastering the HuggingFace Transformers library for loading pre-trained models, the PEFT library for parameter-efficient fine-tuning, and best practices for memory optimization (8-bit/4-bit quantization, gradient checkpointing, gradient accumulation). You'll learn to deploy on Clemson's Palmetto cluster: requesting appropriate GPU resources, setting up environments, managing datasets, and monitoring training jobs."
], img)

# SLIDE 4: What is Multimodal Learning
log_status("What is Multimodal Learning - detailed theory")
img = create_diagram_multimodal_learning()
add_comprehensive_slide("What is Multimodal Learning?", [
    "Definition and Scope: Multimodal learning refers to the process of learning representations and making predictions from data originating from multiple modalities or information channels. A modality represents a particular way information is encoded and experienced—vision (images, videos), language (text, speech), audio (sounds, music), and potentially other sources like sensor data, depth maps, or thermal imaging. Each modality has distinct characteristics: images are spatially structured grids of pixels, text consists of discrete token sequences, and audio comprises continuous temporal waveforms. The fundamental challenge is that these modalities have heterogeneous statistical properties, different dimensionalities, and incompatible native representations.",

    "Theoretical Motivation and Human Cognition: Traditional machine learning operates on homogeneous data—computer vision models process images, NLP models process text, and speech models process audio. However, human intelligence is inherently multimodal. When a child learns about a dog, they simultaneously process its visual appearance, hear the word 'dog' and barking sounds, feel its fur, and integrate all these sensory experiences into a unified conceptual representation. This multimodal integration enables robust recognition (even when some modalities are unavailable) and rich understanding (combining complementary information). Multimodal AI aims to replicate this integrative capability.",

    "Mathematical Formulation: Formally, we have data from M modalities: X = {X₁, X₂, ..., Xₘ}. Each modality Xᵢ ∈ ℝ^(Nᵢ × dᵢ) may have different sample sizes Nᵢ and dimensionalities dᵢ. For example, an image might be 224×224×3 (150,528 dimensions), while its caption is a sequence of tokens with variable length. The goal of multimodal learning is to learn an encoding function f: (X₁, X₂, ..., Xₘ) → Z that maps these heterogeneous inputs into a common representation space Z ∈ ℝᵈ. This shared space must satisfy the crucial property that semantically similar concepts from different modalities have similar representations.",

    "Key Technical Challenges: Several fundamental challenges arise: (1) The modality gap—different modalities have fundamentally different statistical distributions and may not naturally align even for corresponding content; bridging this gap requires learning modality-specific encoders and joint projection functions. (2) Temporal alignment—for time-series modalities like video and audio, determining which temporal segments correspond across modalities is non-trivial. (3) Missing modalities—systems must handle scenarios where some modalities are absent during training or inference. (4) Representation trade-offs—the model must balance preserving modality-specific information versus shared information. (5) Scalability—processing multiple high-dimensional modalities simultaneously requires substantial computational resources."
], img)

# SLIDE 5: Why Multimodal Models
log_status("Why Multimodal Models - applications and impact")
img = create_simple_diagram("Applications", ["VQA", "Captioning", "Generation", "Robotics"])
add_comprehensive_slide("Why Multimodal Models?", [
    "Richer Understanding Through Information Complementarity: Single-modality models face fundamental information bottlenecks. A text-only language model, regardless of size, cannot verify claims about visual content, understand spatial relationships, or ground abstract concepts in perceptual experience. Conversely, vision-only models struggle with abstract reasoning, temporal causality, and tasks requiring world knowledge typically expressed through language. Multimodal models transcend these limitations by integrating complementary information. Vision provides perceptual grounding and spatial understanding; language provides abstract reasoning, world knowledge, and explicit descriptions; audio adds temporal dynamics and affective information.",

    "Transformative Applications Across Domains: Multimodal AI enables entirely new categories of intelligent systems. Visual Question Answering (VQA) allows natural language queries about visual content—essential for accessibility tools serving visually impaired users, educational applications answering student questions about diagrams, medical systems analyzing radiology images with physician queries, and content moderation systems. Image captioning generates textual descriptions automatically, enabling image search engines, alt-text generation for web accessibility, photo organization, and surveillance systems. Text-to-image generation revolutionizes creative workflows—designers specify visual concepts through natural language, artists explore variations rapidly, and educational materials can be illustrated automatically.",

    "Embodied AI and Intelligent Robotics: The development of robots that can operate effectively in human environments fundamentally requires multimodal integration. Consider a home assistant robot: it must understand natural language commands, visually identify and localize objects in cluttered environments, navigate spatially while avoiding obstacles, manipulate objects with appropriate force, and provide verbal feedback. Each of these capabilities involves different modalities. Multimodal models provide the foundational architecture for such embodied AI systems. Recent work like RT-1 and RT-2 from Google demonstrates how vision-language models can be adapted for robotic manipulation.",

    "Robustness, Generalization, and Cross-Modal Transfer: Beyond enabling new applications, multimodal learning often produces more robust and generalizable systems through several mechanisms. Information redundancy provides resilience—when one modality is noisy, ambiguous, or missing, other modalities compensate. Audio-visual speech recognition significantly outperforms audio-only recognition in noisy environments by incorporating lip-reading from visual data. Cross-modal verification improves factual accuracy—a model can check whether generated text is consistent with visual evidence. Research shows that representations learned from multiple modalities often generalize better to new tasks and domains, likely because they capture more fundamental semantic concepts."
], img)

# SLIDE 6: Historical Context
log_status("Historical Context - evolution and breakthroughs")
img = create_simple_diagram("Evolution", ["2017\nAttention", "2019\nBERT+Vision", "2021\nCLIP", "2023\nGPT-4V"])
add_comprehensive_slide("Historical Context and Evolution", [
    "Early Multimodal Systems (pre-2017): The initial era of multimodal learning used separate pre-trained models for each modality, typically combining a convolutional neural network (CNN) for images with recurrent neural networks (RNNs) or early Transformers for text. These models were integrated through simple fusion strategies: late fusion (combining predictions), early fusion (concatenating raw inputs), or intermediate fusion (concatenating hidden representations). Image captioning models like Show and Tell used a CNN to encode images into feature vectors, then fed these as initial states to LSTM decoders. While demonstrating potential, they suffered from limited cross-modal interaction—modalities were processed largely independently until superficial combination at output.",

    "The Attention Revolution and Cross-Modal Fusion (2017-2019): The Transformer architecture introduced by Vaswani et al. in 'Attention Is All You Need' (2017) revolutionized sequence modeling through self-attention mechanisms. Researchers quickly recognized that attention could enable richer multimodal fusion. ViLBERT (Lu et al., 2019) introduced co-attentional Transformer layers where vision and language streams interact through cross-modal attention. LXMERT (Tan & Bansal, 2019) extended this with three Transformer encoders: one for language, one for vision, and one for cross-modality interaction. These models represented images as sequences of region features detected by object detectors, enabling Transformer-style processing. This era established that deep cross-modal interaction throughout the network was key to strong performance.",

    "Contrastive Learning Paradigm Shift (2021-2022): CLIP (Radford et al., 2021) represented a fundamental paradigm shift from task-specific architectures to general-purpose representation learning. Rather than training for specific tasks, CLIP learned broadly useful visual representations by training on matching images to natural language descriptions. The key insight was that internet-scale paired image-text data (400 million pairs) contains rich supervision signals. CLIP used dual encoders—a Vision Transformer for images and a text Transformer—projecting both into a shared 512-dimensional space. Training maximized cosine similarity for matching pairs while minimizing it for non-matching pairs through InfoNCE contrastive loss. The resulting model exhibited remarkable zero-shot capabilities.",

    "Scaling Era and Architectural Innovation (2022-present): Recent development focuses on scaling to larger models and datasets while improving efficiency. Flamingo (Alayrac et al., 2022) demonstrated powerful few-shot learning by freezing both a pre-trained vision encoder and a 70B language model, training only lightweight connector modules with gated cross-attention. GPT-4V (OpenAI, 2023) extended GPT-4's capabilities to images, achieving near-human performance on many benchmarks. Gemini (Google DeepMind, 2023) trained natively multimodal from scratch. Meanwhile, open-source alternatives democratized access: BLIP-2 showed that a lightweight Q-Former could efficiently connect frozen models, and LLaVA demonstrated that instruction-tuning with synthetic data could produce capable vision-language assistants at modest cost."
], img)

print("\n✓ Slides 1-6 completed with comprehensive content")
print("✓ Continuing systematically through all remaining slides (7-78)...\n")

# SLIDE 7: Challenges
log_status("Challenges - technical and theoretical obstacles")
img = create_simple_diagram("Key Challenges", ["Modality\nGap", "Alignment", "Fusion", "Scale"])
add_comprehensive_slide("Challenges in Multimodal Learning", [
    "The Modality Gap Problem: One of the most fundamental challenges is the modality gap—the phenomenon that different modalities have fundamentally different statistical distributions and representational structures that do not naturally align, even for semantically corresponding content. Images are continuous high-dimensional grids where small pixel changes may not alter semantic meaning, while text consists of discrete symbols where changing a single token dramatically alters meaning. Research has shown that even when trained with contrastive objectives to align modalities, there remains a persistent gap in the embedding space. Solutions include learnable temperature scaling in contrastive losses, careful initialization of projection layers, using deeper projection networks, and training with harder negative examples.",

    "Representation Learning and Alignment: Learning representations that effectively bridge modalities while preserving both shared and modality-specific information presents theoretical and practical challenges. The shared representation must capture semantic concepts common across modalities while also preserving modality-specific details important for downstream tasks. This trade-off is governed by the information bottleneck principle: the representation should be maximally informative about the task while being as compressed as possible. Different approaches handle this differently: CLIP prioritizes shared information for zero-shot transfer, while BLIP maintains separate encoders to preserve modality-specific information.",

    "Temporal and Spatial Alignment: For temporal modalities like video and audio, or when aligning text descriptions to specific image regions, determining precise correspondences is non-trivial. Videos and audio may have different frame rates and sampling frequencies—a 10-second video at 30fps has 300 frames while audio at 16kHz has 160,000 samples. Aligning these requires temporal pooling strategies, learned temporal attention, or external alignment from timestamps. For image-text alignment, determining which words correspond to which image regions (grounding) requires either dense supervision or weakly supervised methods that infer alignments from image-caption pairs.",

    "Computational Scalability and Resource Requirements: Processing multiple high-dimensional modalities simultaneously creates severe computational demands. Training CLIP on 400 million image-text pairs required 592 V100 GPUs for 12 days (over 7,000 GPU-days). Scaling to larger datasets and models quickly becomes prohibitively expensive. This computational bottleneck affects both training (requiring massive parallel infrastructure) and inference (larger models have higher latency). Solutions include freezing pre-trained components and training only small connector modules, using quantization to reduce model size, knowledge distillation, and efficient architectures like MoE that activate only subsets of parameters."
], img)

# SLIDE 8: Section - Single Modality Foundations
log_status("Section divider - Single Modality Foundations")
add_section_slide("Part I: Single Modality Foundations")

# SLIDE 9: Vision Modality
log_status("Vision modality - CNNs to Vision Transformers")
img = create_architecture_diagram("Vision Pipeline", ["Image\nInput", "Patch\nEmbedding", "Transformer\nEncoder", "Features"])
add_comprehensive_slide("Single Modality: Vision", [
    "Image Representation and Convolutional Networks: Images are represented as 2D or 3D tensors: H × W × C where H is height, W is width, and C is the number of channels (3 for RGB). A typical image might be 224×224×3, yielding 150,528 raw pixel values. CNNs process images through hierarchical feature extraction: early layers detect low-level features like edges and textures through small convolutional kernels, middle layers combine these into mid-level patterns like object parts, and deeper layers recognize high-level semantic concepts. The inductive biases of CNNs—local connectivity, weight sharing, and translation equivariance—make them highly parameter-efficient for image processing. Architectures like ResNet introduced skip connections enabling very deep networks.",

    "Vision Transformers and Patch-Based Processing: Vision Transformers (ViT) apply the Transformer architecture directly to images by decomposing them into sequences of patches. An image of size H×W×3 is divided into N patches of size P×P, where N = (H×W)/(P×P). Each patch is flattened and linearly projected to embedding dimension d. For example, a 224×224 image with 16×16 patches yields 196 patches, each projected to 768 dimensions. Positional embeddings are added to retain spatial information, and a special [CLS] token is prepended. The sequence of patch embeddings is processed through standard Transformer encoder layers. ViTs scale better than CNNs with increasing data—while they underperform CNNs on smaller datasets, they excel on larger datasets.",

    "Comparative Analysis and Practical Considerations: CNNs and ViTs offer different trade-offs. CNNs have strong inductive biases suitable for smaller datasets but may underfit when given very large datasets. ViTs have less built-in structure, requiring more data to learn visual patterns from scratch, but their flexibility enables better scaling. In practice, most modern multimodal models use ViTs as vision encoders: CLIP uses ViT-B/32 or ViT-L/14, BLIP-2 uses frozen ViT-L/14 from CLIP, LLaVA uses CLIP's ViT encoder. Hybrid architectures like BEiT, MAE, and DINO use self-supervised pre-training on images to improve ViT efficiency.",

    "Feature Extraction for Multimodal Models: For multimodal applications, vision encoders must produce representations compatible with language models. This typically involves: (1) Extracting a fixed-dimensional global representation (the [CLS] token from ViT or final pooled layer from CNN), (2) Extracting a sequence of spatial features (all patch embeddings from ViT or spatial feature map from CNN), or (3) Extracting region-based features using object detectors. CLIP uses the [CLS] token for image-text matching. BLIP-2 uses all patch embeddings, allowing the Q-Former to attend to different image regions. Flamingo uses the Perceiver Resampler to compress patch embeddings into a fixed number of visual tokens."
], img)

# SLIDE 10: Vision Transformers Detail
log_status("Vision Transformers - architecture and scaling")
img = create_architecture_diagram("ViT Architecture", ["Patches", "Projection", "Position", "Transformer", "[CLS]"])
add_comprehensive_slide("Vision Transformers (ViT)", [
    "Architecture and Mathematical Formulation: Vision Transformers treat images as sequences by dividing them into patches. For an image x ∈ ℝ^(H×W×C), we extract N=HW/P² patches of size P×P, flatten each to a vector, and linearly project to dimension d: x_p ∈ ℝ^(N×(P²C)) → E·x_p where E ∈ ℝ^(d×(P²C)). A learnable class token z_0^0 ∈ ℝ^d is prepended, and learnable positional embeddings E_pos ∈ ℝ^((N+1)×d) encode spatial structure. The sequence passes through L Transformer blocks with multi-head self-attention and MLPs: z'_ℓ = MHSA(LN(z_{ℓ-1})) + z_{ℓ-1}; z_ℓ = MLP(LN(z'_ℓ)) + z'_ℓ. The final class token z_L^0 serves as the image representation.",

    "Scaling Properties: ViT shows different scaling behavior than CNNs. On small datasets (ImageNet-1K with 1.3M images), ViT-B/16 underperforms ResNet-50 due to lack of inductive bias. However, with larger pre-training datasets (ImageNet-21K: 14M images, JFT-300M: 300M images), ViT surpasses CNNs. At ViT-H/14 scale with JFT-300M pre-training, accuracy reaches 88.55% on ImageNet—substantially better than CNNs. This demonstrates that ViT trades built-in structure for data efficiency at scale.",

    "Computational Considerations: Self-attention is O(N²d) in sequence length N. For 224×224 images with 16×16 patches, N=196 is manageable. Higher resolutions (448×448 → N=784) quadruple attention cost. Solutions include efficient attention mechanisms (linear, sparse patterns), hierarchical designs (Swin Transformer with local windows), and optimized implementations (FlashAttention). Most multimodal models use standard ViT with 14×14 or 16×16 patches as a practical balance.",

    "Integration in Multimodal Models: ViT dominates modern multimodal architectures. CLIP uses ViT-L/14 producing 256 patch embeddings + 1 CLS token. BLIP-2 freezes CLIP's ViT and connects it via Q-Former. LLaVA uses CLIP's ViT followed by a projection layer to LLM dimensionality. The patch embeddings provide spatially-grounded representations that cross-attention can selectively query, enabling fine-grained visual reasoning. ViT's sequence-based design naturally interfaces with Transformer language models, explaining its widespread adoption."
], img)

# SLIDE 11: Text Modality
log_status("Text modality - tokenization to contextual embeddings")
img = create_architecture_diagram("Text Pipeline", ["Text", "Tokenization", "Embedding", "Transformer", "Output"])
add_comprehensive_slide("Single Modality: Text", [
    "Tokenization and Vocabulary: Text processing begins with tokenization—converting raw strings into discrete tokens. Modern approaches use subword tokenization methods like Byte-Pair Encoding (BPE), WordPiece, or SentencePiece that balance vocabulary size (typically 30K-50K tokens) against representing common words as single tokens while decomposing rare words into subwords. For example, 'unbelievable' might tokenize as ['un', '##believable']. Special tokens mark boundaries: [CLS] (classification), [SEP] (separation), [PAD] (padding), [MASK] (masked language modeling). Each token maps to an integer ID via a learned vocabulary, then to a d-dimensional embedding vector via an embedding matrix E ∈ ℝ^(V×d) where V is vocabulary size.",

    "Contextual Representations via Transformers: Unlike static word embeddings (Word2Vec, GloVe), Transformer-based models produce contextual representations where each token's embedding depends on surrounding context. For input sequence x = [x_1, ..., x_n], we compute initial embeddings h_0 = E[x] + E_pos where E_pos provides positional information. These pass through L Transformer layers: h_ℓ = TransformerBlock(h_{ℓ-1}), producing h_L ∈ ℝ^(n×d) where each position's vector encodes that token in context. The word 'bank' has different representations in 'river bank' versus 'bank account' because self-attention computes interactions with context words.",

    "Model Architectures for Different Objectives: Three main architectures serve different purposes: (1) Encoder-only (BERT): bidirectional context via masked language modeling, suited for understanding tasks like classification and retrieval. (2) Decoder-only (GPT): left-to-right autoregressive generation via next-token prediction, suited for text generation. (3) Encoder-decoder (T5): input encoded bidirectionally, output generated autoregressively, suited for seq2seq tasks. Modern multimodal models typically use encoder-only for text-to-image retrieval (CLIP's text encoder) or decoder-only LLMs for image-to-text generation (LLaVA uses Vicuna, BLIP-2 uses OPT or FlanT5).",

    "Integration with Vision Models: For multimodal learning, text encoders must produce representations compatible with vision. CLIP's text encoder produces a single vector (CLS token) matched against image vectors via cosine similarity. BLIP's text encoder outputs a sequence processed jointly with image features through cross-attention. LLaVA treats images as a sequence of soft prompt tokens prepended to text input for the language model. The choice depends on the task: retrieval benefits from single-vector representations enabling efficient similarity search, while generation requires sequence-based representations enabling autoregressive decoding conditioned on image features."
], img)

# SLIDE 12: Audio Modality
log_status("Audio modality - waveforms to learned representations")
img = create_architecture_diagram("Audio Pipeline", ["Waveform", "Spectrogram", "Features", "Model", "Embeddings"])
add_comprehensive_slide("Single Modality: Audio", [
    "Audio Representation and Feature Extraction: Audio signals are continuous temporal waveforms sampled at rates like 16kHz (16,000 samples/second) or 44.1kHz. Raw waveforms are high-dimensional and redundant. Traditional approaches extract hand-crafted features: spectrograms (time-frequency representations via Short-Time Fourier Transform), mel-spectrograms (frequency bins scaled to mel scale matching human perception), or MFCCs (Mel-Frequency Cepstral Coefficients). Modern approaches learn representations end-to-end from raw waveforms or spectrograms using CNNs (temporal convolutions), RNNs (modeling temporal dependencies), or Transformers (self-attention over time steps or frequency bins).",

    "Self-Supervised Audio Models: Wav2Vec 2.0 pioneered self-supervised audio representation learning. It encodes raw waveforms through CNN layers producing latent representations, quantizes these into discrete codes via product quantization, and trains to predict quantized codes from masked latent representations—analogous to masked language modeling. Pre-training on 960 hours of unlabeled speech (LibriSpeech), then fine-tuning on just 10 minutes of labeled data achieves strong ASR performance, demonstrating learned representations capture rich phonetic and linguistic structure. Audio MAE (Masked Audio Autoencoder) applies masked autoencoding to spectrograms, masking 80% of patches and training to reconstruct them.",

    "Multimodal Audio Integration: Integrating audio with vision and text enables audio-visual speech recognition (lip-reading enhances noisy speech), video captioning with audio (describing both visual content and sounds), and text-to-speech with prosody (generating expressive speech from text). Models like ImageBind learn a shared embedding space across six modalities including audio, enabling audio-to-image retrieval and image-to-audio generation. Audio adds temporal dynamics and affective information (emotion, emphasis) complementing visual and textual modalities."
], img)

# SLIDE 13: Video Modality
log_status("Video modality - spatial and temporal modeling")
img = create_architecture_diagram("Video Pipeline", ["Frames", "Spatial", "Temporal", "Fusion", "Features"])
add_comprehensive_slide("Single Modality: Video", [
    "Video as Spatial-Temporal Data: Video combines spatial structure (each frame is an image) with temporal dynamics (frame sequences capture motion and events). A video V ∈ ℝ^(T×H×W×C) has T frames of H×W×C images. Naive processing (independent frame encoding) loses temporal information. Effective video models must capture both spatial patterns within frames and temporal patterns across frames—objects moving, actions unfolding, scene transitions.",

    "Temporal Modeling Approaches: Several strategies model temporal dynamics: (1) 3D convolutions extending 2D spatial filters to 3D spatial-temporal filters, capturing local motion patterns but computationally expensive. (2) Two-stream networks processing RGB frames (spatial stream) and optical flow (temporal stream) separately then fusing. (3) Recurrent networks (LSTM/GRU) processing frame sequences, capturing long-range dependencies but slow sequential processing. (4) Temporal Transformers (TimeSformer, ViViT) applying self-attention over space-time, enabling global temporal reasoning at quadratic cost O(T²). (5) Factorized attention separating spatial and temporal attention for efficiency.",

    "Video-Language Models: Video understanding requires joint modeling of visual content, motion, and language descriptions. VideoBERT learns visual-linguistic representations from videos and ASR transcripts. CLIP4Clip adapts CLIP for video-text retrieval by encoding videos as sequences of frame features. Flamingo handles interleaved sequences of images and videos with text via gated cross-attention. Video-LLaMA extends LLaMA for video understanding through learned video embeddings. Key challenges include computational cost (10-second video at 1fps = 10 frames; at 224×224 with 16×16 patches, each frame is 196 patches, yielding 1960 visual tokens), temporal alignment (matching moments to sentence clauses), and long-range modeling (understanding narrative arcs)."
], img)

print("\n✓ Slides 7-13 completed with comprehensive content")
print("✓ STATUS: 13/78 slides completed")
print("✓ Continuing with slides 14-30 (Fusion & Contrastive Learning)...\n")

# ============================================================================
# SLIDES 14-30: Multimodal Fusion and Contrastive Learning
# ============================================================================

# SLIDE 14: Section - Multimodal Fusion
log_status("Section divider - Multimodal Fusion Strategies")
add_section_slide("Part II: Multimodal Fusion Strategies")

# SLIDE 15: Early Fusion
log_status("Early Fusion - combining inputs before encoding")
img = create_fusion_diagram()
add_comprehensive_slide("Early Fusion", [
    "Conceptual Framework: Early fusion, also known as input-level fusion or feature-level fusion, combines modalities at the earliest stage by concatenating or combining raw inputs or low-level features before any modality-specific processing. For image and text, this might mean concatenating pixel values with text token embeddings directly. The combined representation then passes through a unified encoder that processes both modalities jointly from the very beginning. The theoretical advantage is that the model can learn cross-modal interactions from the lowest levels, potentially discovering correlations between raw features across modalities that might be lost in later fusion approaches.",

    "Mathematical Formulation and Implementation: Consider vision input x_v ∈ ℝ^(H×W×C) and text input x_t ∈ ℝ^(L×d_t). Early fusion concatenates these: x_combined = concat(flatten(x_v), embed(x_t)) ∈ ℝ^(HWC + Ld_t), then applies a unified encoder: z = Encoder(x_combined). For example, you might flatten an image to a long sequence of pixels and concatenate with text token embeddings, creating a very long sequence processed by a single Transformer. An alternative is to project both to a common dimensionality first: z_v = Linear_v(x_v), z_t = Linear_t(x_t), then concatenate: z = concat(z_v, z_t).",

    "Advantages and Limitations: Early fusion's main advantage is maximizing cross-modal interaction—every layer of the encoder processes combined multimodal information, allowing the model to learn complex joint patterns from the start. However, this comes with significant drawbacks: (1) Computational inefficiency—the unified encoder must process very long sequences (image pixels + text tokens), making self-attention O(N²) prohibitively expensive. (2) Difficulty learning modality-specific features—the model must simultaneously learn appropriate representations for very different data types, which may conflict. (3) Inability to leverage pre-trained modality-specific encoders—you can't use a pre-trained ViT or BERT since the input format is different. (4) Inflexibility—the model requires both modalities at all times; you can't do image-only or text-only inference.",

    "Practical Applications: Early fusion is rarely used in modern large-scale multimodal models due to its limitations. It appears occasionally in specialized scenarios: (1) Small-scale problems where the combined input dimensionality is manageable. (2) Domains with tightly coupled modalities where low-level feature interactions are crucial, such as audio-visual speech where lip movements are tightly synchronized with phonemes. (3) As a baseline for comparison in research. Most successful modern architectures (CLIP, BLIP, Flamingo, LLaVA) avoid early fusion in favor of late fusion or intermediate fusion strategies that process modalities separately initially."
], img)

# SLIDE 16: Late Fusion
log_status("Late Fusion - combining outputs after encoding")
img = create_fusion_diagram()
add_comprehensive_slide("Late Fusion", [
    "Conceptual Framework: Late fusion, also called decision-level fusion, processes each modality through separate encoders until the final stages, then combines the high-level representations or even final predictions. Each modality is encoded independently: z_v = Encoder_v(x_v) and z_t = Encoder_t(x_t), producing modality-specific representations. These representations are then combined through simple operations like concatenation, averaging, or learned weighted combinations, often just before making final predictions. The key principle is that modalities remain separate through most of the processing pipeline.",

    "Mathematical Formulation: For vision encoder f_v and text encoder f_t, we compute independent representations: z_v = f_v(x_v) ∈ ℝ^(d_v) and z_t = f_t(x_t) ∈ ℝ^(d_t). Late fusion combines these for prediction: y = Classifier(concat(z_v, z_t)) or y = Classifier(α·z_v + (1-α)·z_t) where α is learned or fixed. For retrieval tasks, late fusion might compute separate similarity scores then combine them: score = β·sim(z_v, q_v) + (1-β)·sim(z_t, q_t). CLIP uses a variant where encoders produce vectors in the same space: z_v, z_t ∈ ℝ^d, enabling direct similarity comparison: score = cosine(z_v, z_t).",

    "Advantages and Practical Benefits: Late fusion offers several compelling advantages: (1) Computational efficiency—each modality is processed separately, enabling parallel processing and avoiding expensive joint attention over long combined sequences. (2) Leverage pre-trained models—you can use powerful pre-trained encoders for each modality (ViT for images, BERT for text) without modification. (3) Modularity—encoders can be swapped independently, upgraded separately, or trained at different rates. (4) Robustness to missing modalities—if one modality is unavailable, you can still make predictions using available modalities. (5) Scalability—it's straightforward to add new modalities by adding new encoders without redesigning the architecture.",

    "Limitations and When to Use: The main limitation is limited cross-modal interaction—modalities are processed independently, so the model cannot learn fine-grained correspondences between low-level or mid-level features. For example, it cannot learn that 'red' in text should attend to red regions in the image during encoding. This makes late fusion less suitable for tasks requiring detailed cross-modal reasoning like Visual Question Answering ('Is the cat to the left or right of the sofa?'). However, late fusion works excellently for tasks where high-level semantic alignment suffices: image-text retrieval (CLIP), classification with multimodal inputs, and ensembling modality-specific models. Modern architectures often use late fusion for efficiency while adding targeted cross-modal interaction where needed."
], img)

# SLIDE 17: Intermediate Fusion
log_status("Intermediate Fusion - cross-attention and hybrid approaches")
img = create_architecture_diagram("Intermediate Fusion", ["Vision\nEncoder", "Cross\nAttention", "Text\nEncoder", "Joint\nRepresentation"])
add_comprehensive_slide("Intermediate Fusion (Cross-Attention)", [
    "Conceptual Framework and Motivation: Intermediate fusion, also called hybrid fusion or cross-modal fusion, combines the benefits of early and late fusion by allowing modalities to interact at intermediate layers while still maintaining separate encoders for initial processing. Each modality is first encoded through modality-specific layers, then cross-modal interaction layers enable information flow between modalities, and finally modality-specific or joint layers produce final representations. This design allows leveraging pre-trained encoders while adding rich cross-modal reasoning. Cross-attention is the dominant mechanism for intermediate fusion in modern Transformer-based models.",

    "Cross-Attention Mechanism: Cross-attention extends self-attention to relate two different sequences. Given a query sequence from one modality (e.g., text) and a key-value sequence from another modality (e.g., image), cross-attention computes how each query position should attend to the key-value sequence. Mathematically: CrossAttn(Q, K, V) = softmax(QK^T/√d_k)V where Q = W_q·H_text (queries from text), K = W_k·H_image, V = W_v·H_image (keys and values from image). This allows text tokens to selectively attend to relevant image regions. Bidirectional cross-attention computes both text→image and image→text attention, enabling richer interaction.",

    "Architectural Variants: Several architectures employ intermediate fusion differently: (1) ViLBERT uses co-attentional Transformer blocks where vision and language streams have separate self-attention, then cross-attention to the other modality, maintaining two separate but interacting streams throughout. (2) LXMERT has three encoders: vision encoder, language encoder, and cross-modal encoder where cross-attention happens. (3) BLIP uses a multimodal encoder with self-attention that jointly processes concatenated image and text tokens, plus separate unimodal encoders. (4) Flamingo inserts gated cross-attention layers (XATTN-DENSE) into a frozen language model, allowing vision tokens to be attended to by text without modifying the LM's self-attention. (5) BLIP-2's Q-Former uses learnable queries that cross-attend to frozen image features, acting as an information bottleneck.",

    "Trade-offs and Design Choices: Intermediate fusion balances complexity and performance. More cross-modal interaction layers increase model capacity and enable finer-grained reasoning but at computational cost (cross-attention adds O(N_text · N_image · d) operations). The key design choices include: (1) Where to place cross-attention—early layers learn low-level correspondences (textures matching words), late layers learn high-level semantics. (2) Directionality—unidirectional (text attends to image) is cheaper, bidirectional enables richer interaction. (3) Frequency—cross-attention every layer vs. every few layers. (4) Parameter sharing—frozen vs. trainable encoders. Modern practice often freezes powerful pre-trained encoders and adds lightweight cross-attention modules (Flamingo, BLIP-2), achieving strong performance with minimal training cost."
], img)

# SLIDE 18: Attention Mechanisms Recap
log_status("Attention Mechanisms - self, cross, and multi-head attention")
img = create_architecture_diagram("Attention Types", ["Self\nAttention", "Cross\nAttention", "Multi-Head\nAttention"])
add_comprehensive_slide("Attention Mechanisms in Multimodal Models", [
    "Self-Attention Fundamentals: Self-attention computes relationships within a single sequence by allowing each position to attend to all positions including itself. For input sequence x ∈ ℝ^(n×d), we compute queries Q = xW_q, keys K = xW_k, values V = xW_v, then attention: Attn(Q,K,V) = softmax(QK^T/√d_k)V. The softmax(QK^T/√d_k) term produces attention weights α_ij indicating how much position i attends to position j. The scaling by √d_k prevents large dot products that would cause saturation in softmax. Self-attention is permutation-equivariant (position-independent without positional encodings) and has O(n²d) complexity, enabling global reasoning but limiting scalability to very long sequences.",

    "Cross-Attention for Multimodal Integration: Cross-attention relates two different sequences—queries from one sequence attend to keys and values from another. This is crucial for multimodal models: text tokens querying image features to ground language in visual content, or image regions querying text to understand relevant semantic information. The mechanism is identical to self-attention except Q comes from one sequence while K, V come from another: Q = H_text·W_q, K = H_image·W_k, V = H_image·W_v. The attention weights α_ij now indicate how much text token i attends to image region j. Bidirectional cross-attention computes both text→image and image→text attention, enabling symmetric information flow.",

    "Multi-Head Attention and Representation Subspaces: Multi-head attention runs h parallel attention operations with different learned projection matrices, concatenating the results: MultiHead(Q,K,V) = concat(head_1,...,head_h)W_o where head_i = Attn(QW_i^q, KW_i^k, VW_i^v). Each head can learn different types of relationships—one head might focus on spatial proximity, another on semantic similarity, another on specific visual attributes. With h=8 or h=12 heads and total dimension d=768, each head has dimension d/h=64 or 96. This parallel decomposition allows the model to jointly attend to different representation subspaces and different positions, significantly increasing expressiveness without substantially increasing parameters (the projections are smaller).",

    "Efficient Attention Variants: Standard attention's O(n²) complexity becomes prohibitive for long sequences (long documents, high-resolution images, videos). Efficient attention variants reduce this: (1) Sparse attention (Sparse Transformers) restricts attention to local windows or structured patterns, reducing to O(n√n) or O(n log n). (2) Linear attention (Performers, Linear Transformers) approximates softmax attention with kernel methods, achieving O(nd²) complexity linear in sequence length. (3) Memory-compressed attention (Perceiver, Set Transformers) uses a smaller set of learned queries attending to the full input, then subsequent processing uses the compressed representation. (4) FlashAttention optimizes attention computation through memory-efficient GPU kernels without changing the algorithm. Modern multimodal models use these selectively: Flamingo uses the Perceiver Resampler to compress vision features from 256 to 64 tokens before cross-attention."
], img)

# SLIDE 19: Contrastive Learning Introduction
log_status("Contrastive Learning - learning through comparison")
img = create_simple_diagram("Contrastive Learning", ["Positive\nPairs", "Negative\nPairs", "Similarity\nMax/Min", "Learned\nEmbedding"])
add_comprehensive_slide("Contrastive Learning Fundamentals", [
    "Core Principle and Intuition: Contrastive learning is a self-supervised or weakly-supervised learning paradigm that trains models to distinguish between similar (positive) and dissimilar (negative) pairs of examples. The core idea is simple yet powerful: pull together representations of semantically similar items while pushing apart representations of dissimilar items in the embedding space. For multimodal learning, positive pairs are typically matching cross-modal examples (an image and its caption), while negative pairs are mismatched examples (an image and unrelated captions). By learning to maximize similarity for positives and minimize similarity for negatives across many examples, the model learns representations that capture semantic meaning shared across modalities.",

    "Information-Theoretic Foundation: Contrastive learning can be understood through the lens of mutual information maximization. The goal is to learn representations z_v = f_v(x_v) and z_t = f_t(x_t) that maximize mutual information I(z_v; z_t) for corresponding pairs while being invariant to nuisance factors. Maximizing I(z_v; z_t) encourages the representations to contain shared information about the underlying semantic content while discarding modality-specific noise. The InfoNCE loss provides a lower bound on this mutual information, making it tractable to optimize. This theoretical foundation explains why contrastive learning discovers meaningful semantic representations—it's explicitly optimizing for shared information content.",

    "Historical Development: Contrastive learning has deep roots in metric learning and similarity learning but gained prominence with modern deep learning. Early work like Siamese Networks and Triplet Loss (FaceNet) learned embeddings through contrastive objectives for face recognition. SimCLR, MoCo, and BYOL revolutionized self-supervised visual representation learning by showing that contrastive pretraining on unlabeled images rivals or exceeds supervised pretraining. For multimodal learning, the breakthrough was recognizing that naturally co-occurring data (image-caption pairs from the web, video-audio pairs, image-text pairs from alt-text) provides free supervisory signal for contrastive learning at massive scale without expensive manual labeling. CLIP demonstrated that this approach scales to 400 million pairs, learning remarkably general visual representations.",

    "Key Advantages for Multimodal Learning: Contrastive learning is particularly well-suited for multimodal scenarios: (1) Weak supervision—it only requires knowing which image-text pairs correspond, not detailed annotations like bounding boxes or fine-grained labels. This enables training on massive internet-scale datasets cheaply. (2) Flexible objectives—the same contrastive framework works across modalities (vision-text, audio-text, video-text) without task-specific architectural changes. (3) Zero-shot transfer—because the model learns general semantic embeddings rather than task-specific classifiers, it can perform new tasks at test time via similarity comparison (e.g., classifying into categories it never saw during training). (4) Robustness—learning from many negative examples makes representations robust to distribution shift and noise. (5) Scalability—batch-wise contrastive loss enables efficient training with large batch sizes on distributed systems."
], img)

# SLIDE 20: InfoNCE Loss
log_status("InfoNCE Loss - mathematical formulation")
img = create_simple_diagram("InfoNCE", ["Positive\nPair", "Batch\nNegatives", "Softmax", "Loss"])
add_comprehensive_slide("InfoNCE Loss and Contrastive Objectives", [
    "Mathematical Formulation: The InfoNCE (Noise Contrastive Estimation) loss is the standard objective for contrastive learning. For a batch of N image-text pairs {(x_v^i, x_t^i)}_{i=1}^N, we encode them to embeddings {(z_v^i, z_t^i)}_{i=1}^N. For each image i, its corresponding text i is the positive example, while all other texts j≠i in the batch are negatives. The loss for image i is: ℒ_i = -log[exp(sim(z_v^i, z_t^i)/τ) / Σ_{k=1}^N exp(sim(z_v^i, z_t^k)/τ)], where sim(·,·) is typically cosine similarity sim(a,b) = a·b/(||a||||b||) and τ is a temperature parameter. The symmetric loss considers the text-to-image direction: ℒ_t = -log[exp(sim(z_t^i, z_v^i)/τ) / Σ_{k=1}^N exp(sim(z_t^i, z_v^k)/τ)]. The total loss is the average: ℒ = (ℒ_v + ℒ_t)/2.",

    "Temperature Parameter and Its Role: The temperature τ controls the concentration of the distribution and the difficulty of discrimination. Small τ (<0.1) makes the softmax sharper, focusing on the hardest negatives—the model must distinguish the positive from very similar negatives. Large τ (>0.5) makes the distribution more uniform, weighting all negatives more equally. CLIP uses τ=0.07 learned during training. Intuitively, temperature acts as a hyperparameter trading off between easy and hard negative mining. Lower temperatures are generally better for representation quality but can cause training instability if too low. The optimal temperature depends on the embedding dimensionality and the difficulty of the task—harder discrimination tasks benefit from higher temperatures initially.",

    "Understanding Through Gradient Analysis: The gradient of InfoNCE with respect to the image embedding provides insight into what the loss optimizes: ∂ℒ/∂z_v^i = (1/τ)[α_i·z_t^i - Σ_k α_k·z_t^k] where α_k = exp(sim(z_v^i, z_t^k)/τ) / Σ_j exp(sim(z_v^i, z_t^j)/τ) are the attention weights over negatives. The first term pulls z_v^i toward the positive z_t^i. The second term pushes away from a weighted average of all text embeddings, with weights proportional to their current similarity. This means hard negatives (high similarity) receive larger gradients, automatically implementing hard negative mining. This property makes InfoNCE particularly effective—the model focuses on fixing the most confusable pairs.",

    "Practical Considerations and Batch Size Effects: The number of negatives critically impacts learning: more negatives provide stronger supervision and better representations. In a batch of N pairs, each example has N-1 negatives. CLIP used batch size 32,768, providing 32,767 negatives per example—this large batch size was crucial for its success. Implementing large batches requires distributed training with careful synchronization: compute embeddings locally on each device, gather embeddings across devices, compute loss using all embeddings. Gradient accumulation can't replace large batches for contrastive learning because you need embeddings from all samples simultaneously to compute the loss. Alternative strategies for limited computational budgets include: memory banks caching embeddings from previous iterations (MoCo), momentum encoders providing consistent negative representations (MoCo), or hard negative mining selecting the hardest negatives from a larger pool."
], img)

print("\n✓ Slides 14-20 completed with comprehensive content")
print("✓ STATUS: 20/78 slides completed")
print("✓ Continuing with slides 21-30 (CLIP Architecture)...\n")

# SLIDE 21: CLIP Architecture
log_status("CLIP Architecture - dual encoder design")
img = create_clip_architecture()
add_comprehensive_slide("CLIP: Contrastive Language-Image Pre-training", [
    "Architecture Overview and Design Philosophy: CLIP (Contrastive Language-Image Pre-training) introduced by OpenAI in 2021 represents a paradigm shift in vision-language learning. Unlike previous task-specific models trained on carefully curated datasets with expensive annotations, CLIP learns general-purpose visual representations from 400 million image-text pairs collected from the internet. The architecture uses a dual-encoder design: an image encoder (typically a Vision Transformer ViT-L/14 or ResNet) and a text encoder (a Transformer), both projecting to a shared d=512 dimensional embedding space. The key innovation is learning purely through the contrastive objective of matching corresponding image-text pairs, without any task-specific output layers. This enables zero-shot transfer to downstream tasks by simply computing image-text similarity for natural language task descriptions.",

    "Detailed Architecture Components: The vision encoder options include: (1) ResNet-50, ResNet-101 with attention pooling instead of global average pooling, providing better spatial reasoning. (2) ViT-B/32, ViT-B/16, ViT-L/14 where the notation indicates base vs. large model and 32×32 vs. 16×16 vs. 14×14 patch size. The text encoder is a 12-layer Transformer (for ViT-B models) or 12-layer (for ViT-L) with 512 dimensional width, 8 attention heads, and causal self-attention masking. Both encoders are trained from random initialization (no ImageNet pre-training). Each encoder produces a representation: image encoder outputs the [CLS] token (ViT) or attention-pooled features (ResNet), text encoder outputs the [EOS] token representation. These representations are projected to 512 dimensions via learned linear layers: z_v = L_v(Encoder_v(x_v)), z_t = L_t(Encoder_t(x_t)).",

    "Training Procedure and Scale: CLIP training uses the symmetric InfoNCE contrastive loss over image-text similarity matrix. For a batch of N pairs, we compute an N×N similarity matrix S where S_ij = cos(z_v^i, z_t^j). The diagonal contains positive pairs (matching image-text), off-diagonal contains negatives. The loss is sum of image-to-text and text-to-image classification: ℒ = (ℒ_i2t + ℒ_t2i)/2 where each treats the batch as a classification problem. Critically, CLIP uses enormous batch sizes: 32,768 samples per batch, requiring 592 V100 GPUs trained for 12 days (equivalent to 7,104 GPU-days). This large batch size provides 32,767 negatives per positive example, essential for learning high-quality representations. The learned temperature parameter τ (initialized to 0.07) controls the sharpness of the distribution.",

    "Zero-Shot Capabilities and Impact: CLIP's zero-shot performance on ImageNet reaches 76.2% top-1 accuracy without seeing any ImageNet training examples—comparable to the original ResNet-50 trained fully supervised on ImageNet. For arbitrary image classification, you construct text prompts for each class (e.g., 'a photo of a {class}'), encode all prompts, compute similarity between the image and each prompt embedding, and predict the highest similarity class. This works for any set of classes, not just those seen during training. CLIP also excels at retrieval (finding relevant images for text queries or vice versa) and enables creative applications like composing concepts ('a red bicycle' + image regions). The key insight is that natural language supervision from internet data contains incredibly rich and diverse supervision signal, enabling broad generalization despite no task-specific training."
], img)

# SLIDE 22: CLIP Training Process
log_status("CLIP Training - contrastive learning at scale")
img = create_simple_diagram("CLIP Training", ["Image\nBatch", "Text\nBatch", "Encoders", "Similarity\nMatrix", "Contrastive\nLoss"])
add_comprehensive_slide("CLIP Training Process", [
    "Data Collection and Curation: CLIP's training data consists of 400 million image-text pairs collected from the internet, primarily from image alt-text on web pages. Unlike carefully curated datasets like MS-COCO (123K images with 5 human-written captions each) or Conceptual Captions (3.3M pairs with automatic captions), CLIP uses a much larger but noisier dataset. The data collection process searches for images with associated text, filters based on various criteria (resolution, language, duplicates), but deliberately avoids strict curation—the hypothesis being that scale and diversity outweigh cleanliness. This dataset, while not publicly released, demonstrated that weakly supervised learning at sufficient scale produces surprisingly robust and general representations.",

    "Computational Infrastructure and Distributed Training: Training at CLIP's scale requires sophisticated distributed systems. With 592 V100 GPUs and batch size 32,768, each GPU processes approximately 55 samples per batch. The training procedure: (1) Each GPU computes embeddings for its local batch samples (forward pass through encoders and projection layers). (2) Embeddings are gathered across all GPUs via all-gather communication, creating the full batch of 32,768 embeddings on each GPU. (3) Each GPU computes the N×N similarity matrix locally using the full gathered embeddings. (4) The contrastive loss is computed using the rows corresponding to that GPU's samples. (5) Gradients are computed and synchronized via all-reduce, then parameters are updated. This data parallelism with large batch distributed loss computation is essential for contrastive learning—gradient accumulation cannot replace it because you need all embeddings simultaneously to compute the contrastive loss.",

    "Optimization and Hyperparameters: CLIP uses AdamW optimizer with β_1=0.9, β_2=0.98, weight decay 0.2, and a cosine learning rate schedule. The learning rate is warmed up linearly from 0 to the peak value over the first 2,000 steps, then decayed following a cosine schedule. Mixed precision training (FP16) reduces memory footprint and increases throughput. Gradient clipping prevents instability. The temperature parameter τ for the InfoNCE loss is initialized to 0.07 and learned during training (not fixed). Data augmentation is kept minimal: random square crop from rectangular images (maintaining aspect ratio by resizing the shorter side), random horizontal flip. Notably, CLIP doesn't use heavy augmentations common in self-supervised learning (color jittering, blurring, cropping), relying instead on natural diversity in the internet data.",

    "Training Efficiency and Ablations: The paper reports extensive ablations studying design choices. Batch size dramatically affects performance: larger batches (16K-32K) substantially outperform smaller batches (256-2K), confirming that more negatives improve contrastive learning. Vision Transformer architectures scale better than ResNets when increasing model capacity. Text prompt engineering matters: 'a photo of a {class}' works better than just '{class}' for zero-shot classification, likely because training data often contains similar natural language descriptions. Training from scratch (random initialization) outperforms initializing from ImageNet-pretrained weights, suggesting the contrastive objective learns fundamentally different representations than supervised classification. The compute efficiency of CLIP (in terms of data and compute required per unit performance) exceeds specialized supervised models, demonstrating the value of the general-purpose representation learning paradigm."
], img)

# SLIDE 23: CLIP Zero-Shot Learning
log_status("CLIP Zero-Shot - generalizing without fine-tuning")
img = create_simple_diagram("Zero-Shot", ["Class\nNames", "Text\nPrompts", "Encode", "Match\nImage", "Prediction"])
add_comprehensive_slide("Zero-Shot Learning with CLIP", [
    "Zero-Shot Classification Mechanism: Zero-shot learning refers to the ability to perform tasks on categories or concepts the model never encountered during training. CLIP achieves this by leveraging natural language as an interface between visual concepts and task descriptions. For image classification into N classes {c_1, ..., c_N}, the procedure is: (1) Create text prompts for each class, e.g., 'a photo of a {c_i}' or 'a photo of a {c_i}, a type of pet'. (2) Encode all text prompts: {z_t^1, ..., z_t^N} = TextEncoder(prompts). (3) Encode the image: z_v = ImageEncoder(x_v). (4) Compute similarity scores: s_i = cos(z_v, z_t^i) for i=1..N. (5) Predict the class with highest similarity: ŷ = argmax_i s_i. This process requires no fine-tuning or gradient updates—just forward passes through frozen encoders.",

    "Prompt Engineering and Performance: The choice of text prompts significantly impacts zero-shot performance. Simple class names ('cat', 'dog') work but are suboptimal because they lack context that appears in training captions. Adding context ('a photo of a cat') improves performance. Domain-specific prompts help for specialized datasets: 'a satellite photo of {x}' for aerial imagery, 'a blurry photo of {x}' for low-quality images. CLIP uses ensemble prompts: instead of one prompt per class, use multiple templates ('a photo of a {c}', 'a good photo of a {c}', 'a photo of the {c}', etc.) and average their embeddings: z_t^i = mean(TextEncoder(templates)). This ensemble typically improves accuracy by 3-5 percentage points by capturing different phrasings. The optimal prompts can be found through validation or using domain knowledge.",

    "Benchmark Performance and Comparison: CLIP's zero-shot ImageNet performance reaches 76.2% top-1 accuracy (ViT-L/14 model), remarkably close to the original ResNet-50's 76.5% which was trained on all 1.28 million ImageNet training images. On other benchmarks: Stanford Cars 88.4%, Food101 90.8%, Oxford Pets 91.5%. For many specialized datasets, zero-shot CLIP outperforms linear probes (training a logistic regression classifier on frozen features) from ImageNet-pretrained ResNet-50, demonstrating that contrastive learning on diverse internet data produces more transferable representations than supervised learning on ImageNet. However, zero-shot CLIP underperforms on fine-grained classification (flower species, aircraft types) and specialized domains (medical imaging, satellite imagery) where training captions likely don't contain sufficient discriminative language.",

    "Beyond Classification - Retrieval and Compositionality: CLIP's embedding space enables tasks beyond classification. For text-to-image retrieval, encode a text query and find images with highest similarity from a database—useful for search engines, content organization, and image databases. For image-to-text retrieval, encode an image and find most similar text descriptions—useful for automatic tagging, accessibility, and recommendation systems. CLIP exhibits some degree of compositionality: you can combine concepts ('a red cube and a blue sphere') and the embedding reflects both concepts, enabling complex queries. However, compositionality is limited—CLIP struggles with fine-grained spatial relationships ('left of', 'above'), counting, and complex logical combinations, likely because such precise relationships appear infrequently in training captions."
], img)

# SLIDE 24: Section - Advanced Architectures
log_status("Section divider - Advanced Multimodal Architectures")
add_section_slide("Part III: Advanced Multimodal Architectures")

# SLIDE 25: BLIP and BLIP-2
log_status("BLIP and BLIP-2 - unified and efficient architectures")
img = create_architecture_diagram("BLIP-2", ["Frozen\nViT", "Q-Former", "Frozen\nLLM", "Output"])
add_comprehensive_slide("BLIP and BLIP-2", [
    "BLIP: Bootstrapping Language-Image Pre-training: BLIP, introduced by Salesforce in 2022, addresses limitations in existing vision-language models through a unified architecture and synthetic caption generation. Unlike CLIP which only does image-text matching, BLIP uses three objectives: (1) Image-Text Contrastive Learning (ITC) like CLIP for aligned embeddings. (2) Image-Text Matching (ITM) using a multimodal encoder with cross-attention to predict whether image-text pairs match (binary classification), enabling fine-grained understanding. (3) Language Modeling (LM) generating captions autoregressively conditioned on images. This multi-task training creates a versatile model capable of both understanding (retrieval, VQA) and generation (captioning) tasks. BLIP introduces CapFilt: using the model to generate synthetic captions for web images, then filtering noisy captions, creating a larger and cleaner training set. This bootstrapping approach (using the model to improve its own training data) significantly boosts performance.",

    "BLIP-2: Efficient Alignment via Q-Former: BLIP-2 (2023) achieves state-of-the-art vision-language performance with drastically reduced training cost by freezing both the vision encoder and language model, training only a lightweight connector called the Q-Former. The architecture has three components: (1) Frozen image encoder (CLIP ViT-L/14) providing 256 patch embeddings. (2) Q-Former: a learnable querying Transformer with 32 learnable query tokens. The Q-Former has self-attention layers (queries attend to each other) and cross-attention layers (queries attend to frozen image features). It acts as an information bottleneck, extracting the 32 most relevant visual features for language. (3) Frozen LLM (OPT-2.7B, OPT-6.7B, FlanT5-XL, or FlanT5-XXL) that generates text conditioned on Q-Former outputs. The Q-Former is trained with three objectives simultaneously: image-text contrastive learning (queries produce embedding matched to text), image-text matching (predict if image-text match), and image-grounded text generation (queries provide visual context for LLM generation).",

    "Q-Former Architecture and Training: The Q-Former is a 12-layer BERT-base architecture (768 hidden dim) with 32 learnable query tokens initialized randomly. Each layer has: self-attention among queries, cross-attention from queries to image features, and feed-forward network. Critically, different objectives use different attention masks: For ITC loss, queries cannot see text (image-only information). For ITM loss, queries can attend to text via additional cross-attention, enabling fine-grained matching. For LM loss, queries provide context to the frozen LLM via a linear projection matching LLM input dimensions. Training stage 1 connects Q-Former to frozen ViT (no LLM yet) using ITC and ITM losses. Stage 2 connects to frozen LLM using LM loss, keeping ViT frozen. This two-stage approach first learns visual representation then aligns to language.",

    "Performance and Efficiency: BLIP-2 achieves state-of-the-art results on VQA, image captioning, and visual reasoning benchmarks while training only 188M parameters (Q-Former + projection layers) compared to billions in the frozen ViT and LLM. Training takes <10% the compute of comparable models like Flamingo. On VQAv2, BLIP-2 with FlanT5-XXL reaches 82.3% accuracy. On image captioning (COCO), it achieves 144.5 CIDEr score. The efficiency comes from: (1) Freezing expensive components (ViT, LLM) that are already well-trained. (2) Lightweight Q-Former learns alignment cheaply. (3) Information bottleneck (32 queries) compresses image information to essential features for language, reducing LLM computational cost. BLIP-2 demonstrates that you don't need to train giant models from scratch—clever architectural design enables efficient combination of pre-trained components."
], img)

# SLIDE 26: Flamingo
log_status("Flamingo - few-shot visual language understanding")
img = create_architecture_diagram("Flamingo", ["Frozen\nVision", "Perceiver\nResampler", "Gated\nX-Attn", "Frozen\nLM", "Output"])
add_comprehensive_slide("Flamingo: Few-Shot Visual Learning", [
    "Architecture and Design Philosophy: Flamingo, developed by DeepMind (2022), addresses a different challenge than CLIP or BLIP: how can we enable large language models to process visual information while preserving their text-only capabilities and enabling few-shot learning? Flamingo's key insight is to freeze both a pre-trained vision encoder (Normalizer-Free ResNet NFNet) and a large language model (70B Chinchilla), inserting only small trainable modules between them. This allows leveraging massive pre-trained models (requiring person-years to train) while adding vision capabilities cheaply. The architecture interleaves frozen LM blocks with gated cross-attention layers (XATTN-DENSE) that inject visual information. Images/videos are encoded to visual tokens via the Perceiver Resampler, then cross-attended to by text tokens.",

    "Perceiver Resampler and Vision Encoding: Raw vision features from the frozen NFNet or ViT are high-dimensional: a single image might produce 256 or 512 spatial features. Processing these through cross-attention in every LM layer is computationally prohibitive. The Perceiver Resampler solves this by compressing visual information to a fixed small set of tokens (typically 64). It uses a small Transformer with learnable query tokens that cross-attend to the full vision features, producing a compressed visual representation. For videos, temporal attention aggregates information across frames. The resampler outputs are then prepended to text tokens, so from the LM's perspective, an image is just 64 additional tokens of context.",

    "Gated Cross-Attention Layers (XATTN-DENSE): Flamingo inserts a gated cross-attention layer after every frozen LM self-attention layer. The cross-attention allows text tokens to attend to visual tokens: CrossAttn(H_text, V) where H_text are text hidden states and V are visual tokens from Perceiver. Critically, cross-attention outputs are gated: H_out = H_text + tanh(α) ⊙ CrossAttn(H_text, V), where α is a learnable per-layer gate initialized to 0. This initialization is crucial: at the start of training, the gates are closed (tanh(0)=0), so visual information doesn't flow, preserving the LM's text-only behavior. As training progresses, gates gradually open, allowing visual information to influence text generation. This prevents catastrophic forgetting—the LM retains text-only performance while gaining visual understanding.",

    "Few-Shot Learning and Interleaved Data: Flamingo is trained on interleaved image-text sequences from web pages, where images are embedded within text context. This enables few-shot learning at test time: you can provide K example input-output pairs as context, then the model performs the task on a new input. For example, for visual question answering, you might provide 4 examples: (image_1, question_1, answer_1), ..., (image_4, question_4, answer_4), then (image_new, question_new), and Flamingo generates answer_new. With just 4-shot examples, Flamingo achieves 56.3% on VQAv2, competitive with fine-tuned models. This in-context learning capability mimics GPT-3 for text but extends to vision-language tasks. Flamingo's ability to handle arbitrarily interleaved sequences of images and videos with text makes it suitable for complex multimodal documents, conversational agents referencing images, and embodied agents receiving visual observations."
], img)

# SLIDE 27: LLaVA
log_status("LLaVA - instruction tuning for visual assistants")
img = create_architecture_diagram("LLaVA", ["CLIP\nViT", "Linear\nProjection", "Vicuna\nLLM", "Text\nOutput"])
add_comprehensive_slide("LLaVA: Large Language and Vision Assistant", [
    "Architecture Simplicity and Effectiveness: LLaVA (2023) demonstrates that a remarkably simple architecture can produce strong vision-language assistants when combined with high-quality instruction-tuning data. The architecture has just three components: (1) Pre-trained CLIP ViT-L/14 image encoder (frozen during instruction tuning). (2) A single linear projection layer W mapping CLIP visual features to LLM embedding space: H_v = W · z_v where z_v ∈ ℝ^(256×1024) are CLIP patch features and H_v ∈ ℝ^(256×d_LLM) are projected visual tokens. (3) Vicuna-7B or Vicuna-13B (instruction-tuned LLaMA) as the language model. Visual tokens are prepended to text instruction tokens: [H_v, H_text], forming the input to the LLM which generates responses autoregressively. This simple design (far simpler than BLIP-2's Q-Former or Flamingo's gated cross-attention) works because the pre-trained components are already powerful and alignment is relatively straightforward.",

    "Instruction-Tuning Data Generation: LLaVA's key innovation is using GPT-4 to generate high-quality instruction-following data. The process: (1) Start with COCO images and their captions. (2) For each image-caption pair, use GPT-4 (text-only, given the caption as context) to generate diverse instruction-response pairs. For example, given caption 'a dog playing in a park', GPT-4 generates: Question 'What is the dog doing?', Answer 'The dog is playing in the park', or Question 'Describe the image', Answer 'The image shows a dog enjoying playtime in a park setting...'. (3) Generate three types of instructions: conversations (multi-turn Q&A), detailed descriptions (rich captions), and complex reasoning (visual reasoning questions). This produces 158K instruction-response pairs across 58K unique images. The quality is much higher than simple template-based generation because GPT-4 produces natural, diverse, contextually appropriate language.",

    "Training Procedure: LLaVA training has two stages matching BLIP-2's approach: Stage 1 (Feature Alignment): Freeze both ViT and LLM, train only the projection matrix W on 595K image-caption pairs from Conceptual Captions. This teaches the projection how to map CLIP features into the LLM's language space. Training is brief (~4 hours on 8× A100). Stage 2 (Instruction Tuning): Freeze ViT, train both projection W and LLM Vicuna on the 158K GPT-4 generated instruction data. This fine-tunes the LLM to follow visual instructions. Training takes ~24 hours on 8× A100 for LLaVA-13B. The two-stage approach first learns basic visual-to-language mapping, then refines for instruction following. Full training costs are modest compared to training LLMs from scratch—LLaVA-13B requires <$1,000 in compute.",

    "Performance and Impact: LLaVA achieves impressive results on multimodal instruction-following benchmarks. On ScienceQA (science question answering with diagrams), LLaVA-13B reaches 90.92% accuracy, surpassing GPT-3.5 and approaching GPT-4 performance. On conversational tasks and detailed image description, human evaluations show LLaVA responses are preferred over BLIP-2 and on-par with multimodal GPT-4 for many queries. The model can engage in multi-turn conversations about images, answer complex reasoning questions, and provide rich descriptions. LLaVA's significance lies in demonstrating that: (1) Synthetic data generation via strong LLMs (GPT-4) can create high-quality training data cheaply. (2) Simple architectures combined with good data outperform complex architectures with mediocre data. (3) Capable multimodal assistants can be built by academic labs with modest compute budgets, democratizing multimodal AI research."
], img)

# SLIDE 28: GPT-4V and Gemini
log_status("GPT-4V and Gemini - frontier multimodal models")
img = create_simple_diagram("Frontier Models", ["GPT-4V\nMultimodal", "Gemini\nNative", "Proprietary", "State-of-art"])
add_comprehensive_slide("GPT-4V and Gemini: Frontier Models", [
    "GPT-4V: Vision-Enabled GPT-4: GPT-4V (GPT-4 with Vision) extends OpenAI's GPT-4 language model to accept image inputs alongside text. Announced in September 2023, GPT-4V can analyze images, charts, diagrams, screenshots, and documents, answering questions, extracting information, and reasoning about visual content. Performance is exceptional: on MMMU (Multimodal Multitask Understanding) covering college-level questions across subjects, GPT-4V achieves 56.8% accuracy, far exceeding prior models. On MathVista (mathematical reasoning with diagrams), it reaches 49.9%, demonstrating strong visual reasoning. On standard VQA, captioning, and OCR tasks, it approaches or exceeds human performance. However, architectural details remain largely proprietary—OpenAI hasn't published specifics about how vision is integrated with the language model, training procedures, or model size.",

    "Gemini: Natively Multimodal from the Ground Up: Google DeepMind's Gemini (December 2023) takes a different approach: training natively multimodal from the start rather than adapting a text model. Gemini is trained on interleaved sequences of text, images, audio, and video from the beginning, with the hypothesis that joint training enables deeper multimodal understanding than connecting pre-trained components. Three model sizes are released: Gemini Nano (on-device), Gemini Pro (comparable to GPT-3.5), and Gemini Ultra (state-of-the-art). Gemini Ultra achieves 90.0% on MMMU, surpassing GPT-4V and human expert performance (89.8%). On multimodal benchmarks across 30+ tasks, Gemini Ultra achieves state-of-the-art on 20/32. Like GPT-4V, architectural details are limited, but Gemini emphasizes efficient multi-query attention, learned embeddings for different modalities, and training on diverse multimodal data including scientific papers with figures, educational videos, and code with visualizations.",

    "Proprietary vs. Open Models: The frontier models (GPT-4V, Gemini Ultra) remain closed-source: model weights, training data, and detailed architectures are not public. This creates a significant gap between proprietary capabilities and open research. Advantages of proprietary models: enormous compute budgets (potentially millions of GPU-hours), access to vast proprietary datasets, extensive safety tuning and red-teaming. Disadvantages: lack of transparency, inability for researchers to reproduce or analyze, potential biases unknown, limited customization. The open-source community responds with models like LLaVA, Fuyu, Qwen-VL, CogVLM, and Idefics, which while not matching frontier performance, provide transparency and accessibility. The trade-off between open and closed development continues to shape the field's trajectory.",

    "Capabilities and Limitations: Frontier models exhibit impressive capabilities: understanding complex diagrams and charts, performing OCR and document analysis, visual reasoning and commonsense understanding, multi-turn conversation referencing images, some degree of spatial reasoning and counting. However, limitations persist: fine-grained spatial relationships ('Which object is leftmost?'), precise counting ('How many apples?'), temporal reasoning in videos, factual accuracy (models can hallucinate details not present in images), and bias (often reflecting biases in training data). Safety concerns include generating harmful content from images, privacy issues with recognizing individuals, and potential for misuse in misinformation. Ongoing research focuses on improving reasoning, reducing hallucinations, better handling of complex multi-image inputs, and alignment with human values."
], img)

# SLIDE 29: Comparison of Architectures
log_status("Architecture Comparison - trade-offs across approaches")
img = create_simple_diagram("Architectures", ["CLIP\nDual", "BLIP\nCross", "Flamingo\nFrozen", "Gemini\nNative"])
add_comprehensive_slide("Comparing Multimodal Architectures", [
    "Dual-Encoder Models (CLIP): Dual encoders process modalities completely independently, producing embeddings in a shared space. Advantages: (1) Computational efficiency—parallel encoding, no expensive cross-attention. (2) Modularity—swap encoders independently. (3) Scalability—easy to add modalities. (4) Pre-trained encoder reuse. (5) Excellent for retrieval—symmetric embeddings enable efficient similarity search. Limitations: (1) No fine-grained cross-modal interaction during encoding. (2) Struggles with tasks requiring detailed reasoning (VQA, spatial relationships). (3) Limited compositionality. Best for: Image-text retrieval, zero-shot classification, embedding-based tasks. Training cost: Medium to high (large batches required). Examples: CLIP, ALIGN, BASIC.",

    "Cross-Encoder Models (BLIP, ViLBERT): Cross-encoders use intermediate fusion with cross-attention, allowing modalities to interact deeply. Advantages: (1) Rich cross-modal reasoning—text attends to relevant image regions, vice versa. (2) Strong performance on VQA, visual reasoning, complex understanding tasks. (3) Flexible for both understanding and generation. Limitations: (1) Computationally expensive—cross-attention is O(N_text × N_image × d). (2) Not optimized for retrieval (need to encode every image-text pair together). (3) Cannot leverage pre-trained encoders as easily. Best for: VQA, visual reasoning, image captioning, tasks requiring fine-grained understanding. Training cost: High. Examples: BLIP, ViLBERT, LXMERT, UNITER.",

    "Frozen-Backbone Models (BLIP-2, Flamingo, LLaVA): These freeze large pre-trained vision and/or language models, training only small connector modules. Advantages: (1) Training efficiency—train <1% of parameters. (2) Leverage massive pre-trained models without retraining. (3) Preserve pre-trained capabilities (LLM text performance, ViT vision performance). (4) Enables few-shot learning (Flamingo) or instruction-following (LLaVA). Limitations: (1) Performance ceiling from frozen components—can't adapt them to multimodal data. (2) Modality gap may persist if alignment module is too simple. (3) Requires very strong pre-trained components. Best for: Scenarios with limited compute, instruction-following assistants, leveraging existing LLMs for vision. Training cost: Low to medium. Examples: BLIP-2, Flamingo, LLaVA.",

    "Native Multimodal Models (Gemini): Training from scratch on multimodal data without separate pre-training stages. Advantages: (1) Deep integration—model learns optimal representations for multimodal understanding from the start. (2) No modality gap from aligning separate pre-trained models. (3) Potentially better long-term performance ceiling. Limitations: (1) Enormous computational requirements—must learn vision, language, and alignment simultaneously. (2) Requires massive diverse multimodal datasets. (3) Can't leverage existing pre-trained models. (4) Less accessible to academic labs. Best for: Organizations with vast compute and data resources aiming for maximum performance. Training cost: Very high (millions of GPU-hours). Examples: Gemini, CM3, potentially GPT-4V."
], img)

# SLIDE 30: Section - Training Strategies
log_status("Section divider - Training and Fine-tuning")
add_section_slide("Part IV: Training and Fine-Tuning Strategies")

print("\n✓ Slides 21-30 completed with comprehensive content")
print("✓ STATUS: 30/78 slides completed (38% done!)")
print("✓ Continuing with slides 31-50 (Training strategies and fine-tuning)...\n")

# ============================================================================
# SLIDES 31-50: Training Strategies and Fine-Tuning
# ============================================================================

# SLIDE 31: Pre-training Objectives
log_status("Pre-training Objectives - learning from data")
img = create_simple_diagram("Objectives", ["Contrastive\nLearning", "Masked\nModeling", "Generation", "Matching"])
add_comprehensive_slide("Pre-training Objectives", [
    "Contrastive Learning Objectives: Contrastive objectives train models to distinguish between matching and non-matching cross-modal pairs. The InfoNCE loss ℒ_NCE = -log[exp(sim(z_i, z_j)/τ) / Σ_k exp(sim(z_i, z_k)/τ)] maximizes similarity for positive pairs (i,j) while minimizing similarity to negatives k. This objective excels at learning aligned embeddings for retrieval but doesn't explicitly teach generation or fine-grained understanding. Variants include: symmetric vs. asymmetric loss (CLIP uses symmetric), temperature scheduling (starting high, decreasing), and hard negative mining (selecting the hardest negatives from a larger pool). The batch size critically affects performance—CLIP's success relied on 32K batch sizes providing 32K-1 negatives per sample.",

    "Masked Modeling Objectives: Inspired by BERT's masked language modeling, masked modeling randomly masks portions of inputs and trains the model to reconstruct them. For images: Masked Image Modeling (MIM) masks random patches and trains to predict pixel values or discrete visual tokens (BEiT, MAE). For text: Masked Language Modeling (MLM) masks tokens and predicts them from context. For multimodal: Masked Cross-Modal Modeling masks text tokens and predicts from image context, or masks image regions and predicts from text. BLIP uses masked language modeling conditioned on images. These objectives teach the model to build rich representations by forcing it to infer missing information from available context, encouraging deep understanding rather than shallow pattern matching.",

    "Generation and Matching Objectives: Image captioning objectives train models to generate text descriptions autoregressively: ℒ_LM = -Σ_t log P(w_t | w_{<t}, image). This teaches the model to produce coherent language grounded in visual content. Visual Question Answering similarly generates answers conditioned on images and questions. Image-Text Matching (ITM) is a binary classification objective: given an image-text pair, predict whether they match (positive pair from dataset) or not (negative pair created by random sampling). ITM with cross-attention enables fine-grained understanding as the model must verify detailed correspondences, not just high-level semantic similarity. BLIP combines ITC (contrastive), ITM (matching), and LM (generation) in multi-task training, creating versatile models.",

    "Combining Multiple Objectives: Modern models often train with multiple objectives simultaneously to leverage complementary supervision signals. BLIP uses ITC + ITM + LM. BLIP-2's Q-Former trains with ITC + ITM for stage 1, then adds LM for stage 2. The rationale: contrastive learning produces aligned embeddings good for retrieval, matching enables fine-grained reasoning, and generation teaches producing fluent language. Multi-task training requires balancing loss weights, training stability (some objectives may converge faster), and computational efficiency (generation objectives are more expensive than matching). The optimal combination depends on target applications—retrieval-focused applications weight contrastive losses higher, while generation-focused applications emphasize language modeling losses."
], img)

# SLIDE 32: Data Requirements
log_status("Data Requirements - scale and quality")
img = create_simple_diagram("Data Scale", ["100K\nCurated", "10M\nFiltered", "400M\nWeb-Scale", "5B\nLargest"])
add_comprehensive_slide("Data Requirements and Scaling", [
    "Dataset Size and Diversity: Modern multimodal models rely on massive datasets. CLIP trained on 400M image-text pairs from internet alt-text. ALIGN used 1.8B pairs with minimal filtering. Flamingo trained on billions of interleaved image-text examples from web pages. This scale represents orders of magnitude beyond earlier datasets: MS-COCO (123K images), Conceptual Captions (3.3M), even ImageNet (1.2M images). The hypothesis: while individual training samples may be noisy, the aggregate statistical signal from hundreds of millions of diverse examples enables learning robust, general representations that capture the full distribution of real-world visual and linguistic phenomena. Diversity matters as much as size—training on varied domains, styles, concepts, and languages produces better generalization than training on larger but homogeneous data.",

    "Data Quality vs. Quantity Trade-offs: Early multimodal datasets (MS-COCO, Visual Genome) featured high-quality human annotations: precise object bounding boxes, detailed captions, relationship annotations. Creating such datasets is expensive (months of annotation work, thousands of dollars). In contrast, web-scraped datasets like CLIP's are noisy: captions may be uninformative ('IMG_1234.jpg'), incorrect, or tangentially related to images. However, research shows that at sufficient scale, quantity overwhelms quality—CLIP's noisy 400M pairs outperforms models trained on 10× cleaner but 100× smaller datasets. The explanation: statistical learning can average over noise when given enough data, and diversity of weak supervision provides richer signal than limited strong supervision. However, for specialized domains (medical imaging, scientific diagrams), curated data remains important.",

    "Filtering and Curation Strategies: Even web-scale datasets benefit from basic filtering. Common strategies: (1) Image quality filters (resolution > 200×200, aspect ratio reasonable, file size constraints). (2) Text quality filters (language detection, minimum/maximum length, removing pure URLs). (3) Diversity filters (deduplication based on image hashing or text n-gram overlap). (4) Safety filters (removing NSFW content, copyrighted material, personal information). (5) Concept balancing (ensuring diverse concepts rather than over-representing common objects like 'person' or 'outdoor'). CLIP used basic filters but deliberately avoided aggressive curation. BLIP introduced CapFilt: using the model itself to generate synthetic captions and filter noisy original captions, bootstrapping data quality during training. The optimal filtering depends on computational budget and target applications.",

    "Data Augmentation Techniques: Data augmentation artificially increases dataset diversity. For vision: random cropping (teaching translation invariance), horizontal flipping (mirror invariance), color jittering (lighting robustness), random rotations (rotation invariance), cutout/dropout (occlusion robustness). For text: back-translation (translating to another language and back), paraphrasing via language models, word substitution with synonyms. For multimodal: cross-modal augmentation like replacing captions with synthetic ones, or image cropping forcing the model to match partial images to full captions. However, CLIP found minimal augmentation worked best—just random cropping and flipping—relying on natural diversity in web data rather than artificial augmentation. Heavy augmentation can help with smaller datasets but may degrade performance or introduce artifacts with web-scale training."
], img)

# SLIDE 33: Training from Scratch
log_status("Training from Scratch - large-scale pre-training")
img = create_simple_diagram("Training", ["Initialize", "Batch", "Forward", "Loss", "Backward", "Update"])
add_comprehensive_slide("Training Multimodal Models from Scratch", [
    "Initialization Strategies: Training from scratch means random initialization of all parameters (except potentially positional embeddings). For Vision Transformers, CLIP initializes weights following truncated normal distributions with carefully tuned standard deviations. Embedding matrices use smaller initialization (stddev=0.02), projection layers use larger (stddev=0.01). Surprisingly, CLIP found that random initialization outperforms ImageNet pre-training for vision encoders—the contrastive objective learns fundamentally different representations than supervised classification, and starting from supervised weights provides minimal benefit. For text encoders, random initialization also works, though some works initialize from BERT checkpoints. The key is proper initialization scale: too small causes slow learning, too large causes instability.",

    "Computational Requirements and Infrastructure: Training at scale requires extensive computational infrastructure. CLIP used 592 V100 GPUs for 12 days, equivalent to 7,104 GPU-days (approximately 170,000 GPU-hours). At modern cloud pricing (~$2-3/hour per V100), this represents ~$400K in compute costs. Distributed training across hundreds of GPUs requires: data parallelism (each GPU processes different batch samples), all-gather operations to collect embeddings for contrastive loss, all-reduce for gradient synchronization, high-bandwidth interconnects (InfiniBand) between nodes, and careful load balancing. Mixed precision training (FP16 or BF16) roughly doubles throughput by reducing memory bandwidth and enabling larger batches. Gradient checkpointing trades computation for memory, enabling larger models but increasing training time 20-30%.",

    "Optimization and Hyperparameters: Standard optimizers include AdamW (Adam with decoupled weight decay) with typical hyperparameters: β_1=0.9, β_2=0.98 or 0.999, weight decay 0.1-0.2, and gradient clipping to prevent spikes. Learning rate is crucial: CLIP uses cosine decay from peak (1e-3 to 5e-4 depending on model size) to near-zero, with linear warmup over 2,000-10,000 steps. The warmup prevents early instability when gradients are large and variable. Batch size affects both convergence and final performance—larger batches provide more stable gradients and better representations for contrastive learning, but too large can harm generalization (though this effect is weaker for contrastive objectives than supervised learning). Training duration varies: CLIP trained for ~30 epochs over 400M pairs; smaller datasets may train for 100+ epochs.",

    "Monitoring and Debugging: Training runs lasting days or weeks require careful monitoring. Key metrics: training loss (should decrease smoothly), validation metrics (retrieval recall@1, zero-shot accuracy), gradient norms (detecting exploding/vanishing gradients), learning rate schedule, throughput (samples/second), GPU utilization. Common failure modes: loss not decreasing (learning rate too low or too high), loss exploding (gradient clipping insufficient, learning rate too high), underfitting (model too small, training too short), overfitting (large model on small data, though rare for web-scale training), dead neurons (units with zero gradient, from poor initialization), mode collapse (model learns degenerate solutions). Checkpointing every N steps enables recovery from failures. Weight averaging over last K checkpoints (EMA) often improves final performance."
], img)

# SLIDE 34: Fine-Tuning Approaches
log_status("Fine-Tuning - adapting pre-trained models")
img = create_simple_diagram("Fine-Tuning", ["Pre-trained\nModel", "Task Data", "Update\nWeights", "Adapted\nModel"])
add_comprehensive_slide("Fine-Tuning Multimodal Models", [
    "Full Fine-Tuning: Full fine-tuning updates all model parameters on task-specific data. Starting from a pre-trained model (e.g., CLIP), you continue training on your target dataset (e.g., medical images with radiology reports) with a task-specific objective (e.g., image-report matching). The model adapts its representations to the target domain. Benefits: maximal adaptation capability, can significantly improve task performance. Challenges: (1) Computational cost—backpropagating through the entire model requires substantial memory and compute. (2) Risk of catastrophic forgetting—the model may lose general capabilities learned during pre-training if fine-tuned too aggressively. (3) Overfitting risk on small datasets—millions of parameters tuned on thousands of examples. (4) Storage—need separate copies for each task. Best practices: use lower learning rates than pre-training (1e-5 to 1e-4), shorter training (few epochs), early stopping, and maintain a held-out validation set.",

    "Linear Probing: Linear probing freezes all pre-trained layers and trains only a linear classifier on top. For CLIP, you freeze both vision and text encoders and train a logistic regression layer: y = softmax(W·z_v + b) on labeled images. This tests how good the frozen representations are for the target task without any adaptation. Benefits: extremely fast and cheap (trains in minutes), no risk of forgetting, useful for evaluating representation quality. Limitations: cannot adapt to domain shift, limited by quality of frozen features. Performance on linear probing often predicts few-shot and fine-tuning performance—models with better linear probe accuracy typically improve more with fine-tuning. Linear probing serves as a strong baseline and quality test.",

    "Partial Fine-Tuning and Layer-wise Learning Rates: An intermediate approach freezes early layers (which learn general features) and fine-tunes later layers (which learn task-specific features). For ViT, you might freeze the first 6 layers and tune the last 6. Layer-wise learning rate decay assigns decreasing learning rates to earlier layers: if layer L has LR = 1e-4, layer L-1 might have LR = 0.9 × 1e-4, layer L-2 has 0.9² × 1e-4, etc. This reflects that early layers need less adaptation. Benefits: reduces computation (fewer parameters updated), reduces overfitting risk, preserves general features while adapting high-level representations. Research shows diminishing returns from tuning more layers—often tuning the last 25-50% of layers captures most of the benefit of full fine-tuning at a fraction of the cost.",

    "Few-Shot and Zero-Shot Adaptation: Pre-trained models like CLIP enable zero-shot performance (no task-specific training) via prompt engineering. Few-shot learning uses a handful of labeled examples (5-100) to adapt. Approaches: (1) Few-shot prompting for models like Flamingo (provide examples in-context). (2) Adapter modules trained on few examples while freezing the backbone. (3) Meta-learning methods (MAML, Prototypical Networks) that learn how to quickly adapt from few examples. (4) Prompt tuning learns continuous prompt embeddings rather than discrete words. These approaches are crucial when labeled data is scarce or expensive (medical domains, rare languages, specialized tasks). They leverage the general knowledge in pre-trained models while requiring minimal task-specific supervision."
], img)

# SLIDE 35: Parameter-Efficient Fine-Tuning (PEFT)
log_status("PEFT - efficient adaptation strategies")
img = create_simple_diagram("PEFT Methods", ["LoRA", "Adapters", "Prompt\nTuning", "Prefix\nTuning"])
add_comprehensive_slide("Parameter-Efficient Fine-Tuning (PEFT)", [
    "Motivation and Foundations: As models scale to billions or trillions of parameters, full fine-tuning becomes prohibitively expensive. A 7B parameter model in FP32 requires 28GB just to store weights, plus activation memory during training, often exceeding even high-end GPU memory (A100 has 80GB). Fine-tuning for multiple tasks requires storing separate full copies. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small subset of parameters (0.1-5% of total) while keeping most weights frozen. The key insight from intrinsic dimensionality research: the optimization landscape has low effective dimensionality—most parameters can be frozen and only a low-dimensional subspace needs updating to achieve good task adaptation. PEFT methods differ in which parameters they train and how.",

    "Adapter Modules: Adapters insert small trainable modules between frozen Transformer layers. A typical adapter has: (1) Down-projection to low dimension: h_down = W_down·h ∈ ℝʳ where r << d (r=64, d=768). (2) Non-linearity (ReLU or GeLU). (3) Up-projection back to original dimension: h_out = W_up·h_down ∈ ℝᵈ. (4) Residual connection: output = h + h_out. The adapter has 2×d×r parameters (e.g., 98K for r=64, d=768). Inserting adapters after each attention and FFN layer in a 12-layer model adds ~1.2M parameters vs. 100M+ for the full model (<1%). Adapters train via standard backpropagation, gradients flow through frozen layers but only adapter weights update. Performance is comparable to full fine-tuning on many tasks while being far more efficient.",

    "Prompt and Prefix Tuning: Instead of adding modules, these methods prepend learnable vectors to model inputs. Prompt tuning (for encoders): learn continuous prompt embeddings P ∈ ℝ^(k×d) prepended to input tokens: [P, x_1, x_2, ..., x_n]. The model processes this as a longer sequence but only P is updated. Prefix tuning (for decoders): prepend learnable key-value pairs to each attention layer, providing task-specific context without modifying other parameters. With k=10-20 virtual tokens and d=768, prompt tuning adds only 7K-15K parameters. Performance scales with model size—for models >1B parameters, prompt tuning matches full fine-tuning, but for smaller models (<300M), the gap is larger. The mechanism likely works by steering the model's behavior through the attention mechanism, similar to how natural language prompts steer zero-shot behavior.",

    "Low-Rank Adaptation (LoRA) - Preview: LoRA, which we'll cover in detail next, decomposes weight updates into low-rank matrices. Instead of updating W → W + ΔW where ΔW ∈ ℝ^(d×d), LoRA represents ΔW = BA where B ∈ ℝ^(d×r) and A ∈ ℝ^(r×d) with rank r << d. With r=8, you train 2×d×r=12K parameters per weight matrix instead of d²=589K. LoRA typically applies to attention projection matrices (W_q, W_k, W_v, W_o) in each layer. Compared to adapters, LoRA adds no latency at inference (matrices can be merged: W_new = W + BA), while adapters add small overhead from extra layers. LoRA has become the dominant PEFT method for LLMs and multimodal models due to its efficiency, performance, and zero inference overhead."
], img)

# SLIDE 36: LoRA - Low-Rank Adaptation
log_status("LoRA - mathematical foundations and implementation")
img = create_lora_diagram()
add_comprehensive_slide("LoRA: Low-Rank Adaptation in Detail", [
    "Mathematical Foundation: LoRA is based on the hypothesis that weight updates during fine-tuning have low intrinsic rank—the effective dimensionality of parameter changes is much smaller than the full parameter space. Instead of learning a full-rank update ΔW ∈ ℝ^(d×d) with d² parameters, LoRA constrains updates to low rank r by decomposing: ΔW = B·A where B ∈ ℝ^(d×r), A ∈ ℝ^(r×d), and r << d. The forward pass becomes: h' = W·x + ΔW·x = W·x + B·(A·x). Training only updates B and A (2dr parameters) while freezing W. The rank r controls capacity: r=1 is very constrained (learning a rank-1 perturbation), r=64 is more flexible, r=d reduces to full fine-tuning. Empirically, r=8 to r=32 suffices for most tasks, representing 0.1-1% of parameters.",

    "Implementation Details: LoRA is typically applied to attention matrices in Transformers: W_q (query projection), W_k (key projection), W_v (value projection), and W_o (output projection). For a model with L layers, 4 attention matrices per layer, d=768, and r=8, total trainable parameters = L × 4 × 2 × d × r = 12L × d × r. For a 12-layer model, this is ~590K parameters vs. 85M+ for full fine-tuning. Initialization matters: B is initialized to zero (B=0), A is initialized randomly (typically Gaussian). This ensures ΔW=BA=0 at start, so the model begins at the pre-trained solution and gradually adapts. The scaling factor α/r (where α is a hyperparameter, often α=16) controls the magnitude of updates. During inference, W and BA can be merged: W_merged = W + BA, eliminating any additional latency.",

    "Advantages and Limitations: LoRA advantages: (1) Parameter efficiency—train <1% of parameters. (2) Memory efficiency—less activation memory, smaller optimizer states. (3) Zero inference latency—merge weights after training. (4) Multiple task handling—store many LoRA modules for different tasks, swap at inference. (5) Composability—can combine multiple LoRA modules. (6) Performance—matches or approaches full fine-tuning on most tasks. Limitations: (1) Not all tasks benefit equally—some require full-rank updates. (2) Rank selection is a hyperparameter (though r=8-16 works well generally). (3) May underperform full fine-tuning on tasks requiring large distribution shift. (4) Primarily benefits large models—gains are smaller for small models where full fine-tuning is already cheap.",

    "Practical Usage with HuggingFace PEFT: The HuggingFace PEFT library makes LoRA easy to use: (1) Load base model: model = AutoModel.from_pretrained('model-name'). (2) Define LoRA config: config = LoRAConfig(r=8, lora_alpha=16, target_modules=['q_proj', 'v_proj'], task_type='CAUSAL_LM'). (3) Wrap model: model = get_peft_model(model, config). (4) Train normally—only LoRA parameters have requires_grad=True. (5) Save adapters: model.save_pretrained('lora_adapters'). (6) Load for inference: model = AutoModel.from_pretrained('base-model'); model = PeftModel.from_pretrained(model, 'lora_adapters'). This workflow enables researchers to fine-tune billion-parameter models on single GPUs and share small adapter files (MBs) instead of multi-GB full models."
], img)

print("\n✓ Slides 31-36 completed with comprehensive content")
print("✓ STATUS: 36/78 slides completed (46% done!)")
print("✓ Continuing with slides 37-50...\n")

# Continue with slides 37-50 - Practical Implementation and Evaluation

# SLIDE 37: Implementation Tools
log_status("Implementation Tools - HuggingFace ecosystem")
img = create_simple_diagram("Tools", ["Transformers", "PEFT", "Datasets", "Accelerate"])
add_comprehensive_slide("Implementation Tools and Libraries", [
    "HuggingFace Transformers Library: The Transformers library provides unified APIs for loading and using thousands of pre-trained models. Key components: (1) AutoModel.from_pretrained('model-name') automatically loads the correct architecture and weights. (2) AutoProcessor/AutoTokenizer handles input preprocessing. (3) pipeline() provides high-level APIs for common tasks. For multimodal models: CLIPModel, BlipModel, Blip2Model, LlavaForConditionalGeneration. Example: model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14'); processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14'); inputs = processor(text=['a cat', 'a dog'], images=images, return_tensors='pt'); outputs = model(**inputs). The library handles tokenization, image preprocessing, batching, and device placement automatically.",

    "PEFT Library for Efficient Fine-Tuning: The PEFT (Parameter-Efficient Fine-Tuning) library implements LoRA, adapters, prompt tuning, and other efficient methods. Workflow: (1) from peft import get_peft_model, LoraConfig. (2) config = LoraConfig(r=8, lora_alpha=16, target_modules=['q_proj', 'v_proj'], lora_dropout=0.1). (3) model = get_peft_model(base_model, config). (4) Train normally - only PEFT parameters update. (5) model.save_pretrained('lora_weights') saves only adapter weights (~MBs). (6) Load with PeftModel.from_pretrained(). Supports quantization integration: loading 8-bit or 4-bit base models with bitsandbytes, then adding LoRA for fine-tuning large models on consumer GPUs.",

    "Datasets and Data Loading: HuggingFace Datasets library provides streaming access to large datasets, efficient caching, and preprocessing. Common multimodal datasets available: COCO (load_dataset('HuggingFaceM4/COCO')), Conceptual Captions, Visual Genome, VQAv2, MSCOCO-Captions. Features: streaming (process without downloading entire dataset), map() for parallel preprocessing, built-in caching, conversion to PyTorch/TensorFlow. Example: dataset = load_dataset('coco', split='train', streaming=True); processed = dataset.map(preprocess_function, batched=True). For custom data, use ImageFolder or load from CSV/JSON with from_dict() or from_pandas().",

    "Training Infrastructure: Accelerate library simplifies distributed training: (1) accelerator = Accelerator() automatically detects multi-GPU, TPU, or single GPU. (2) model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader) wraps components for distributed training. (3) accelerator.backward(loss) handles gradient accumulation. (4) Works with DeepSpeed, FSDP for large model training. Trainer API provides high-level training loop: define TrainingArguments (learning rate, batch size, epochs), instantiate Trainer(model, args, train_dataset, eval_dataset), call trainer.train(). Handles checkpointing, logging (WandB, TensorBoard), evaluation, early stopping automatically. For custom training loops, use PyTorch Lightning or raw PyTorch with manual optimization."
], img)

# SLIDE 38: Palmetto Cluster Setup
log_status("Palmetto Cluster - Clemson HPC resources")
img = create_simple_diagram("Palmetto", ["Login\nNode", "Submit\nJob", "GPU\nNode", "Results"])
add_comprehensive_slide("Using Clemson's Palmetto Cluster", [
    "Cluster Architecture and Resources: Palmetto is Clemson's HPC cluster with 2,000+ compute nodes, including GPU nodes with V100, A100, and RTX-series GPUs. Login via SSH: ssh username@login.palmetto.clemson.edu. Storage: (1) Home directory (/home/username, 100GB quota, backed up) for code and small files. (2) Scratch space (/scratch/username, multi-TB quota, not backed up) for datasets and checkpoints. (3) Group storage (/project/group_name) for shared data. Always work from scratch for training - it has better I/O performance and no quota concerns. Never store large datasets in home directory.",

    "Requesting GPU Resources: Use PBS (Portable Batch System) to request compute nodes. Interactive session: qsub -I -l select=1:ncpus=8:ngpus=1:gpu_model=a100:mem=64gb,walltime=4:00:00. This requests 1 node with 8 CPUs, 1 A100 GPU, 64GB RAM for 4 hours. Batch jobs: write PBS script with resource requests and commands, submit with qsub job_script.pbs. GPU options: gpu_model=v100 (16GB VRAM), gpu_model=a100 (40GB or 80GB), gpu_model=rtx6000 (24GB). For multi-GPU: ngpus=4. For distributed training across multiple nodes: select=2:ncpus=16:ngpus=4:gpu_model=a100 requests 2 nodes with 4 GPUs each (8 GPUs total).",

    "Environment Setup: Palmetto uses module system for software. Load CUDA and conda: module load cuda/11.8 anaconda3. Create environment: conda create -n multimodal python=3.10; conda activate multimodal. Install packages: pip install torch torchvision transformers accelerate peft datasets pillow wandb. For faster installation, use: pip install --no-cache-dir to avoid disk quota issues. Set cache directories to scratch: export HF_HOME=/scratch/username/.cache/huggingface; export TORCH_HOME=/scratch/username/.cache/torch. This prevents filling home directory with model downloads. For custom environments, use singularity containers for reproducibility.",

    "Running Training Jobs: Example batch script: #!/bin/bash, #PBS -N multimodal_training, #PBS -l select=1:ncpus=8:ngpus=1:gpu_model=a100:mem=64gb, #PBS -l walltime=24:00:00, module load cuda/11.8 anaconda3, source activate multimodal, cd /scratch/username/project, python train_multimodal.py --config config.yaml. Monitor jobs: qstat -u username. View output: cat job_name.o<jobid>. Cancel jobs: qdel <jobid>. For long training, use checkpointing to resume if wall time exceeded. Use screen or tmux for interactive sessions to prevent disconnection. Log metrics to WandB for remote monitoring: export WANDB_API_KEY=your_key; wandb login."
], img)

# SLIDE 39: Memory Optimization
log_status("Memory Optimization - fitting large models")
img = create_simple_diagram("Memory Tricks", ["Quantization", "Gradient\nCheckpointing", "Mixed\nPrecision", "Accumulation"])
add_comprehensive_slide("Memory Optimization Techniques", [
    "Quantization: Reducing Model Precision: Quantization represents weights and activations with fewer bits. FP32 (32-bit float) uses 4 bytes per parameter; FP16 (half precision) uses 2 bytes; INT8 uses 1 byte; INT4 uses 0.5 bytes. For a 7B parameter model: FP32 = 28GB, FP16 = 14GB, INT8 = 7GB, INT4 = 3.5GB. Loading in 8-bit: from transformers import BitsAndBytesConfig; config = BitsAndBytesConfig(load_in_8bit=True); model = AutoModel.from_pretrained('model-name', quantization_config=config). Quantization reduces memory but may slightly degrade performance. QLoRA (Quantized LoRA) loads base model in 4-bit, adds LoRA in FP16, enabling fine-tuning 65B models on single 48GB GPU. Trade-off: memory savings vs. small quality loss (typically <1% for INT8, 1-3% for INT4).",

    "Gradient Checkpointing: Trading Compute for Memory: Gradient checkpointing (activation checkpointing) saves memory by not storing all intermediate activations during forward pass. Instead, re-compute them during backward pass when needed. Memory savings: ~50-70% activation memory. Speed cost: ~20-30% slower training (extra forward computations). Enable in HuggingFace: model.gradient_checkpointing_enable(). Essential for training large models - without it, a 7B model might need 80GB+ for activations; with it, fits in 40GB. Combine with other techniques: gradient checkpointing + mixed precision + 8-bit optimizer enables training 13B models on 24GB GPUs.",

    "Mixed Precision Training: FP16/BF16 Acceleration: Mixed precision uses FP16 (or BF16) for most computations while keeping FP32 for numerical stability where needed. Benefits: (1) 2× memory reduction (FP16 vs. FP32). (2) 2-3× speedup on modern GPUs (Tensor Cores accelerate FP16). (3) Enables larger batch sizes. FP16 has limited range, can underflow/overflow. BF16 (Brain Float 16) has same range as FP32 but less precision, better for training stability. PyTorch: use torch.cuda.amp.autocast() for automatic mixed precision. HuggingFace Trainer: TrainingArguments(fp16=True) or (bf16=True). Modern A100 GPUs prefer BF16; older V100s prefer FP16. Loss scaling prevents underflow in FP16 by scaling gradients up before backward, then scaling down before optimizer step.",

    "Gradient Accumulation: Simulating Larger Batches: Gradient accumulation performs multiple forward-backward passes before optimizer step, simulating larger batch sizes. If GPU memory allows batch_size=4 but you want effective_batch_size=32, use accumulation_steps=8: perform 8 iterations, accumulating gradients, then optimizer step. In HuggingFace: TrainingArguments(per_device_train_batch_size=4, gradient_accumulation_steps=8). Trade-off: same memory as small batch, same convergence as large batch, but slower (no parallelism across accumulation steps). Note for contrastive learning: gradient accumulation doesn't replace large batches because you need all embeddings simultaneously in loss computation. Works well for language modeling, supervised learning, but limited for contrastive objectives."
], img)

# SLIDE 40: Evaluation Metrics
log_status("Evaluation Metrics - measuring performance")
img = create_simple_diagram("Metrics", ["Retrieval\nR@K", "Generation\nCIDEr", "VQA\nAccuracy", "Zero-Shot\nTransfer"])
add_comprehensive_slide("Evaluation Metrics for Multimodal Models", [
    "Retrieval Metrics: For image-text retrieval, compute similarity between all images and texts in test set, rank by similarity. Recall@K (R@K): percentage of queries where the correct match is in top K results. R@1 is strictest (exact top match), R@5 and R@10 are more lenient. Report both directions: image-to-text (given image, retrieve caption) and text-to-image (given caption, retrieve image). Median rank (MedR): median position of correct match in ranking. Lower is better. Mean reciprocal rank (MRR): average of 1/rank for correct matches. NDCG (Normalized Discounted Cumulative Gain) weights higher-ranked results more. Example CLIP on Flickr30K: R@1=88.0%, R@5=98.7%, R@10=99.4% for image-to-text retrieval.",

    "Generation Metrics: For image captioning, compare generated captions to human references. BLEU (Bilingual Evaluation Understudy): precision of n-gram overlap with references. BLEU-4 considers up to 4-grams. Range 0-100, higher is better. Weakness: doesn't handle paraphrases well. METEOR: like BLEU but includes synonyms and stemming. CIDEr (Consensus-based Image Description Evaluation): TF-IDF weighted n-gram similarity, correlates well with human judgments. Most commonly reported for captioning. SPICE (Semantic Propositional Image Caption Evaluation): parses captions into scene graphs, compares objects, attributes, relations. Best correlation with human judgments but slower to compute. Example BLIP-2 on COCO: CIDEr=144.5, BLEU-4=40.4.",

    "Visual Question Answering Metrics: VQA is typically formulated as classification (select from answer set) or generation (generate free-form answer). For VQAv2 benchmark, accuracy = min(#humans_gave_answer/3, 1). This accounts for answer ambiguity - if 2/3 humans give answer 'yes' and model predicts 'yes', accuracy is 2/3. Soft accuracy is more forgiving than exact match. For open-ended VQA, use generation metrics (BLEU, METEOR) or human evaluation. Some tasks use multiple-choice accuracy. Example BLIP-2 on VQAv2: 82.3% accuracy. For compositional reasoning, report per-category accuracy (counting, color recognition, spatial relationships) to identify strengths/weaknesses.",

    "Zero-Shot and Transfer Metrics: Zero-shot accuracy: performance on classification task without any training on those classes. Computed by encoding class names as text prompts, computing similarities with test images. ImageNet zero-shot accuracy is standard benchmark. Transfer learning: fine-tune on downstream task, measure task accuracy. Linear probe: freeze features, train linear classifier, measures representation quality. Few-shot learning: train on K examples per class (K=1, 5, 10), measure test accuracy. Calibration: measure whether model confidence (predicted probability) matches actual accuracy. Expected Calibration Error (ECE) quantifies calibration. Robustness: measure performance under distribution shift (ImageNet-A, ImageNet-R, ImageNet-Sketch) or adversarial perturbations."
], img)

# SLIDE 41: Section - Applications
log_status("Section divider - Real-World Applications")
add_section_slide("Part V: Real-World Applications")

# SLIDE 42: Visual Question Answering
log_status("VQA - answering questions about images")
img = create_architecture_diagram("VQA System", ["Image", "Question", "Multimodal\nModel", "Answer"])
add_comprehensive_slide("Visual Question Answering (VQA)", [
    "Task Formulation and Datasets: VQA requires understanding both image content and natural language questions to produce correct answers. Questions range from simple object recognition ('What color is the car?') to complex reasoning ('Why is the person smiling?'). Major datasets: VQAv2 (1.1M questions on 200K images, balanced to prevent language bias), GQA (22M questions with compositional structure and scene graphs), OK-VQA (outside knowledge required, not just visual content), VizWiz (images from blind users, real-world challenges). Answer types: yes/no, number, color, object, attribute, activity. Models must handle ambiguity - same question on different images needs different answers.",

    "Model Architectures for VQA: Early approaches: concatenate image features (CNN) with question embeddings (LSTM/GRU), predict answer with classifier. Modern approaches use cross-attention: encode image to patch features, encode question to token embeddings, use cross-attention for question tokens to attend to relevant image regions. BLIP uses multimodal encoder with both self-attention and cross-attention. BLIP-2 uses Q-Former to extract visual features, then LLM generates answer. LLaVA treats VQA as text generation: prepend image tokens to question, generate answer autoregressively. Attention visualization reveals which image regions the model focuses on when generating answers, providing interpretability.",

    "Challenges and Recent Progress: Challenges: (1) Language bias - models exploit question statistics ('What color is the sky?' → 'blue' without looking at image). VQAv2 balances questions to penalize this. (2) Compositional reasoning - multi-step reasoning ('Is the person to the left of the cat wearing a hat?'). GQA tests this explicitly. (3) Counting - 'How many people?' is challenging for attention-based models. (4) Outside knowledge - 'What monument is this?' requires world knowledge beyond visual perception. OK-VQA tests this. (5) Spatial reasoning - 'above', 'left of', 'between' are difficult. Recent progress: Large models (GPT-4V, BLIP-2-FlanT5-XXL) reach 82-85% on VQAv2, approaching human performance (>90%). Chain-of-thought prompting improves reasoning: 'Let's think step-by-step' before answer.",

    "Practical Applications: Accessibility tools for visually impaired users - describe images and answer questions ('What's in this photo?', 'Read the text on this sign'). E-commerce - answer customer questions about products ('Is this shirt cotton or polyester?'). Education - interactive tutoring systems answering questions about diagrams, charts, historical images. Healthcare - radiology assistants answering physician questions about X-rays, MRIs. Content moderation - automated systems answering policy questions ('Does this image contain violence?'). Document understanding - answering questions about scanned documents, receipts, forms. All benefit from models that can understand both visual and textual information to provide accurate, context-aware answers."
], img)

# SLIDE 43: Image Captioning
log_status("Image Captioning - describing visual content")
img = create_architecture_diagram("Captioning", ["Image", "Encoder", "Decoder", "Caption"])
add_comprehensive_slide("Image Captioning and Description", [
    "Task Definition and Challenges: Image captioning generates natural language descriptions of image content. Unlike VQA which has questions as input, captioning must decide what to describe. Challenges: (1) Salient selection - what objects/events to mention? (2) Detail level - brief caption ('a dog') vs. detailed ('a golden retriever playing fetch in a park on a sunny day'). (3) Language diversity - avoiding generic descriptions ('a person standing'). (4) Hallucination - describing objects not present. (5) Bias - over-describing certain attributes (e.g., mentioning gender when irrelevant). Datasets: MS-COCO (123K images, 5 captions each), Flickr30K (31K images), Conceptual Captions (3.3M image-caption pairs from web), NoCaps (novel object captioning testing compositional generalization).",

    "Model Architectures and Training: Classic encoder-decoder: CNN encodes image to fixed vector, LSTM/GRU decoder generates caption word-by-word conditioned on image vector. Weakness: fixed image representation can't attend to different regions for different words. Attention-based captioning: decoder attends to different spatial regions (CNN feature map or ViT patches) at each decoding step. When generating 'dog', attend to dog region; when generating 'park', attend to background. Modern Transformer-based: encode image to sequence of patch embeddings, decoder Transformer generates caption with cross-attention to image patches. BLIP uses both contrastive learning and language modeling for captioning. Training: teacher forcing (use ground truth previous words during training), then at inference use model's own predictions (exposure bias problem). Solutions: scheduled sampling, reinforcement learning with SCST (Self-Critical Sequence Training) optimizing CIDEr directly.",

    "Controllable and Dense Captioning: Standard captioning is unconstrained - model decides what to describe. Controllable captioning allows specifying: (1) Length - short vs. detailed captions. (2) Style - formal, casual, poetic. (3) Focus - describe particular objects or regions. (4) Perspective - describe from certain viewpoint. Implementation: add control codes to decoder input (e.g., [LENGTH=20] [STYLE=formal]) or use conditional training. Dense captioning generates multiple region-specific descriptions for a single image - detecting salient regions and captioning each. Useful for detailed image understanding. Visual paragraph generation produces longer, coherent descriptions with multiple sentences and narrative structure, describing overall scene, individual objects, and relationships.",

    "Applications and Impact: Accessibility - automatic alt-text for images on websites, helping visually impaired users understand visual content. Screen readers use captions to describe images. Content organization - automatically tagging and organizing photo libraries. Google Photos, Apple Photos use captioning for search ('find photos of dogs in parks'). Social media - auto-suggesting captions for uploaded images. News and journalism - generating initial descriptions for wire photos. E-commerce - automatically describing products from images. Video understanding - extending to video captioning for content summarization, video search. Medical imaging - generating radiology reports from X-rays, CT scans (though requiring specialized training for medical accuracy). Visual dialog - captioning is often first step in conversational systems about images."
], img)

# SLIDE 44: Text-to-Image Generation
log_status("Text-to-Image Generation - creating from descriptions")
img = create_simple_diagram("T2I", ["Text\nPrompt", "Model", "Generated\nImage"])
add_comprehensive_slide("Text-to-Image Generation", [
    "From Retrieval to Generation: While CLIP and similar models learn aligned image-text embeddings for retrieval and classification, text-to-image generation synthesizes novel images from text descriptions. Early approaches: GANs conditioned on text (StackGAN, AttnGAN) generated low-resolution, often distorted images. Breakthrough: DALL-E (2021) and DALL-E 2 (2022) using autoregressive Transformers (DALL-E) or diffusion models (DALL-E 2) on web-scale data produced high-quality, diverse, creative images. Stable Diffusion (2022) open-sourced diffusion-based generation. Midjourney and DALL-E 3 pushed artistic quality further. These models can generate photorealistic images, artistic styles, novel compositions ('a robot painting a portrait'), and abstract concepts.",

    "Diffusion Models for Text-to-Image: Diffusion models dominate current text-to-image generation. Training: (1) Forward diffusion - gradually add Gaussian noise to images over T steps (e.g., T=1000), destroying structure. (2) Reverse diffusion - train neural network to predict and remove noise at each step, conditioned on text. Architecture: U-Net denoiser with cross-attention to text embeddings from CLIP or T5. At inference: start from pure noise, iteratively denoise for T steps, conditioning on text prompt. Classifier-free guidance improves text alignment: generate both conditional (with text) and unconditional (without text) predictions, extrapolate further in conditional direction: ε = ε_uncond + w(ε_text - ε_uncond) where w>1 increases text influence. Latent diffusion (Stable Diffusion) operates in compressed latent space from VAE, reducing computation.",

    "Capabilities and Limitations: Impressive capabilities: photorealism, artistic styles (impressionism, pixel art, oil painting), novel combinations ('avocado chair'), conceptual blending, variations on existing images (inpainting, outpainting, image editing), consistent style across variations. Limitations: (1) Text-image alignment - models sometimes miss details in prompts, especially negation ('without') and spatial relationships ('left of'). (2) Counting and numbers - generating exactly 5 objects is unreliable. (3) Text rendering in images - generated text is often gibberish. (4) Body/hand anatomy - fingers, limbs sometimes malformed. (5) Bias - reflects training data biases (e.g., 'CEO' generates predominantly males). (6) Memorization - may reproduce training images, raising copyright concerns. Ongoing research addresses these through better architectures, training objectives, and data curation.",

    "Applications and Ethical Considerations: Creative applications: concept art for games/films, graphic design, marketing materials, personal art creation, rapid prototyping of visual ideas. Product design - visualizing concepts before physical prototypes. Education - generating custom educational illustrations. Content creation - stock photos, social media content. Research tool - generating synthetic training data. Ethical concerns: (1) Copyright and attribution - models trained on artists' work without compensation. (2) Misinformation - generating fake but realistic images for deception. (3) Deepfakes - creating fake images of real people. (4) Bias perpetuation and amplification. (5) Economic impact on artists and illustrators. (6) Content moderation challenges. Responses: watermarking generated images, usage policies restricting harmful content, licensing models for commercial use, technical detection methods for generated images, ongoing policy discussions."
], img)

# SLIDE 45: Visual Reasoning
log_status("Visual Reasoning - complex understanding tasks")
img = create_simple_diagram("Reasoning", ["Observation", "Question", "Inference", "Answer"])
add_comprehensive_slide("Visual Reasoning and Understanding", [
    "Beyond Perception to Reasoning: Visual reasoning goes beyond recognizing objects to understanding relationships, inferring hidden properties, and logical deduction. Tasks: (1) Spatial reasoning - 'Is object A to the left of object B?', 'What is between the car and the building?' (2) Temporal reasoning in videos - 'What happened before the person fell?', understanding cause-effect. (3) Analogical reasoning - 'If image A relates to B as C relates to D, what is D?' (4) Physical reasoning - 'Will this tower of blocks fall?', 'Which glass contains more water?' (5) Social reasoning - 'Why is this person happy?', understanding emotions, social situations. (6) Commonsense reasoning - 'Is this a safe place to swim?' using world knowledge beyond visual features.",

    "Datasets and Benchmarks: CLEVR (Compositional Language and Elementary Visual Reasoning) - synthetic scenes with geometric objects, questions requiring multi-step reasoning ('What is the color of the cube to the left of the blue sphere?'). Tests compositional generalization. GQA (balanced, compositional visual questions with scene graphs). Visual Genome (scene graphs with objects, attributes, relationships). VCR (Visual Commonsense Reasoning) - multiple choice questions requiring inferencing beyond image content. Winoground (visio-linguistic compositional reasoning) - hard negatives testing fine-grained understanding. NLVR2 (Natural Language Visual Reasoning) - true/false statements about pairs of images. Bongard-LOGO (few-shot concept learning and reasoning).",

    "Model Approaches: Modular networks decompose reasoning into sub-tasks: attend to 'cube', filter 'blue', find 'left of', query 'color'. Each module is a neural network. Compositional execution mimics structured reasoning. Neural state machines maintain and update state representations through reasoning steps. Transformer-based models with enough capacity can learn to reason end-to-end without explicit modules. Large vision-language models (GPT-4V, Gemini) show strong reasoning through scale and training on diverse data. Chain-of-thought prompting improves reasoning: asking model to explain step-by-step before answering. Program synthesis approaches generate executable programs (in Python or DSL) from questions, execute on image to get answer, providing interpretability.",

    "Challenges and Frontiers: Current models often exploit dataset biases rather than truly reasoning - may fail on out-of-distribution examples. Systematic generalization remains difficult: models trained on 'red cube left of blue sphere' struggle with 'blue cube left of red sphere' despite systematic structure. Causal reasoning poorly developed: models correlate co-occurrences but don't understand causality. Abstract reasoning (Raven's Progressive Matrices, Bongard problems) challenging for current architectures. Temporal reasoning in videos requires modeling long-range dependencies and event understanding. Integrating vision with external knowledge graphs for knowledge-intensive reasoning is active research area. Neuro-symbolic approaches combining neural perception with symbolic reasoning show promise but not yet scaled successfully. Future progress likely requires better inductive biases, hybrid architectures, and richer training signals beyond static image-text pairs."
], img)

# SLIDE 46: Document Understanding
log_status("Document Understanding - OCR and layout analysis")
img = create_simple_diagram("Document", ["Scan/Image", "Layout", "OCR", "Structure", "Understanding"])
add_comprehensive_slide("Document and Scene Text Understanding", [
    "From OCR to Understanding: Traditional OCR (Optical Character Recognition) extracts text from images but loses spatial layout, structure, and visual context. Modern document understanding requires: (1) Text detection - localizing text regions. (2) Text recognition - converting regions to strings (OCR). (3) Layout analysis - identifying document structure (titles, paragraphs, tables, figures). (4) Visual-semantic understanding - combining visual appearance, text content, and structure for comprehension. Applications: form understanding, receipt parsing, invoice processing, table extraction, diagram understanding, scene text reading (street signs, menus, products).",

    "Multimodal Document Models: LayoutLM family embeds both text tokens and 2D positional information (bounding boxes) in Transformer models, pre-trained on document images and text. Understands layout patterns ('title at top', 'tables have grid structure'). Donut (Document Understanding Transformer) processes document images end-to-end without separate OCR - encoder reads image, decoder generates structured output (JSON with fields extracted). Pix2Struct pre-trains on screenshots and HTML for better web page/diagram understanding. GPT-4V and Gemini handle documents multimodally - can read, understand, and answer questions about PDFs, forms, receipts, charts. These models combine vision (understanding visual layout, handwriting, fonts) with language (semantic understanding of text).",

    "Applications: Automated data entry from forms - insurance claims, tax returns, surveys. Receipt/invoice processing - expense management, accounting automation. Medical record processing - extracting information from clinical notes, prescriptions, lab reports. Legal document analysis - contract review, due diligence. Academic paper understanding - parsing figures, tables, equations. Accessibility - making scanned documents accessible to visually impaired users. ID verification - extracting information from passports, driver licenses. Business card scanning. Menu reading for navigation apps. Real-time translation of signs, menus via camera. These tasks benefit from multimodal understanding combining visual layout cues with text semantics.",

    "Challenges: Handwriting recognition - high variability across individuals. Low-quality scans - blur, skew, poor lighting. Multi-lingual documents - mixing languages, scripts. Complex layouts - multi-column, irregular structures. Tables with spanning cells, merged rows. Mathematical equations and specialized notation. Diagrams with embedded text. Curved or distorted text (on products, signs). Small text sizes. Text with unusual fonts or styling. Noisy backgrounds. Current models handle clean, structured documents well but struggle with challenging real-world conditions. Ongoing work on robustness, generalization across domains, zero-shot adaptation to new document types, and integration with large language models for deeper reasoning about document content."
], img)

# SLIDE 47: Video Understanding
log_status("Video Understanding - temporal modeling")
img = create_simple_diagram("Video", ["Frames", "Temporal\nModel", "Events", "Understanding"])
add_comprehensive_slide("Video Understanding and Temporal Modeling", [
    "Extending to Temporal Dimension: Video adds temporal dynamics to visual understanding - motion, events, activities, narratives. Key differences from images: (1) Temporal dependencies - understanding requires relating information across time. (2) Motion - object movements, camera motion. (3) Events - actions with temporal extent ('running', 'cooking'). (4) Causality - earlier events cause later events. (5) Long-range dependencies - understanding narratives requires minutes of context. Computational challenges: (10-second video at 30fps = 300 frames, each potentially 224×224×3 = 150K pixels, totaling 45M input values before any encoding).",

    "Video-Language Models: VideoBERT extends BERT to video-text by tokenizing video into discrete codes (via clustering) and treating as 'visual sentences'. CLIP4Clip adapts CLIP for video retrieval - encodes video as sequence of frame features, aggregates via mean pooling or temporal Transformer. Flamingo handles arbitrarily interleaved images and videos with text through gated cross-attention, supporting video understanding and generation. Video-LLaMA connects video encoder to LLaMA language model via learned projection layers, enabling video question answering. Training typically uses web videos with captions, transcripts (YouTube), instructional videos (HowTo100M), or annotated datasets (ActivityNet, Kinetics, MSR-VTT for video captioning).",

    "Tasks and Applications: Video classification - categorizing clips into activities (UCF-101, Kinetics). Temporal action localization - finding when actions occur ('eating' from 00:23-00:45). Video captioning - generating descriptions of video content. Video question answering - answering questions about video events. Moment retrieval - finding clips matching text queries. Video summarization - extracting key moments. Action prediction - forecasting future actions from partial observations. Applications: video search engines, content moderation, sports analytics (automatically identifying plays, tracking players), surveillance (anomaly detection, activity recognition), video editing automation, accessibility (describing video content for visually impaired), education (analyzing teaching videos), healthcare (analyzing surgical videos, patient movements).",

    "Challenges and Recent Progress: Computational cost - processing long videos is expensive. Efficient strategies: sparse sampling (sample few frames), multi-scale processing (low-res for full video, high-res for salient moments), memory-efficient attention. Long-range dependencies - standard Transformers struggle with hundreds of frames. Approaches: hierarchical models (short clips → clip features → full video), memory mechanisms (storing past in compressed form), state space models (efficient long-sequence modeling). Temporal grounding - precisely localizing events in long videos. Cross-modal temporal alignment - matching spoken words or text to video moments. Large foundation models (GPT-4V, Gemini) show impressive video understanding by training on diverse video-text data, but temporal reasoning remains weaker than spatial reasoning. Ongoing work on better temporal representations, efficient architectures, and richer temporal supervision signals."
], img)

# SLIDE 48: Multimodal Embeddings
log_status("Multimodal Embeddings - unified representations")
img = create_simple_diagram("Embeddings", ["Image", "Text", "Audio", "Video", "Shared\nSpace"])
add_comprehensive_slide("Unified Multimodal Embeddings", [
    "Beyond Pairwise Alignment: Early multimodal models aligned two modalities (image-text). Recent work extends to many modalities in a unified embedding space. ImageBind learns a single embedding space for images, text, audio, depth, thermal, and IMU (inertial measurement) data. Training uses naturally paired data: videos pair images with audio, ego-centric videos pair images with IMU, depth sensors pair RGB with depth. By chaining these natural pairings through images as a 'hub', all modalities end up in aligned embedding space. Benefits: zero-shot cross-modal retrieval (find images from audio, audio from text, etc.), compositional reasoning across modalities, emergent capabilities (combining modalities not paired during training).",

    "Architecture and Training: Separate encoders for each modality: ViT for images, Transformer for text, similar for others. All project to d=1024 dimensional shared space. Training: pairwise contrastive losses for naturally co-occurring modalities. For example, image-audio pairs from videos: ℒ_IA = InfoNCE(z_image, z_audio). Image-text pairs: ℒ_IT = InfoNCE(z_image, z_text). Image-depth pairs: ℒ_ID = InfoNCE(z_image, z_depth). No direct text-audio or text-depth pairing needed - transitivity through images aligns them. This 'hub' approach scales to M modalities with only M-1 pairwise losses (connecting each to hub) rather than M(M-1)/2 all-pairs losses. Emergent zero-shot transfer: after training only image-audio and image-text, text-to-audio retrieval works despite never training on text-audio pairs directly.",

    "Applications and Capabilities: Cross-modal retrieval - find images matching audio descriptions, find audio matching text. Cross-modal generation - text → audio → image pipeline. Multimodal search - query with any modality, retrieve any modality. Robotics - unifying visual, tactile, proprioceptive, and language inputs for embodied AI. Medical imaging - aligning CT, MRI, X-ray, ultrasound, and radiology reports in shared space. Accessibility - translating between modalities (e.g., audio descriptions for images, image captions for audio). Multimodal reasoning - composing cues from multiple modalities for robust understanding. Any-to-any translation potential through shared representation space.",

    "Challenges and Future Directions: Modality-specific information loss - shared embedding space may discard modality-unique information important for some tasks. Modality gap persists even with careful alignment - embeddings from different modalities occupy different regions despite semantic similarity. Balancing generality vs. specificity. Computational cost - training many encoders and modality pairs. Limited naturally paired data for some modality pairs (e.g., thermal-audio). Extension to more modalities: EEG, fMRI for brain signals, chemical sensors, radar, genetic data in biology. Temporal alignment across modalities. Continuous learning - adding new modalities without retraining from scratch. Interpretability - understanding what information is preserved vs. discarded in shared space. Despite challenges, unified multimodal embeddings are promising direction toward general-purpose perceptual representations."
], img)

# SLIDE 49: Robustness and Fairness
log_status("Robustness and Fairness - reliability and ethics")
img = create_simple_diagram("Ethics", ["Robustness", "Fairness", "Bias", "Safety"])
add_comprehensive_slide("Robustness, Fairness, and Ethical Considerations", [
    "Robustness to Distribution Shift: Models trained on curated datasets (COCO, ImageNet) may fail on real-world distribution shifts. Types: (1) Domain shift - different imaging conditions (lighting, blur, compression). ImageNet-C tests corruption robustness. (2) Style shift - sketches vs. photos, renderings vs. real images. ImageNet-Sketch, ImageNet-R test this. (3) Natural adversarial examples - realistic but challenging images. ImageNet-A contains naturally hard examples. (4) Adversarial perturbations - imperceptible noise causing misclassification. Models struggle with: (1) Out-of-distribution inputs - unfamiliar domains, rare events. (2) Compositional generalization - novel combinations of familiar concepts. (3) Adversarial attacks. Improving robustness: training on diverse data, data augmentation, adversarial training, domain adaptation, test-time adaptation, ensemble methods. CLIP shows better robustness than supervised models due to diverse web training data.",

    "Bias and Fairness: Multimodal models can perpetuate and amplify societal biases present in training data. Types: (1) Representation bias - some groups underrepresented (e.g., non-Western cultures, elderly, disabled people). (2) Association bias - stereotypes (e.g., 'nurse' retrieving female images more than male). (3) Performance disparities - higher error rates on minority groups (skin tone bias in face recognition). (4) Allocative harm - unfair resource distribution based on biased model predictions. (5) Quality-of-service harm - degraded performance for some users. Examples: image search amplifying gender stereotypes, captioning systems misidentifying race, VQA systems reflecting cultural biases. Concerns extend to generated content: text-to-image models may generate biased or stereotypical images for prompts like 'CEO' or 'nurse'.",

    "Measuring and Mitigating Bias: Measurement: (1) Dataset audits - analyzing training data demographics, stereotypical associations. (2) Performance stratification - testing accuracy across demographic groups. (3) Association tests - measuring biased correlations (e.g., 'cooking' with women). (4) Counterfactual evaluation - testing if changing protected attributes changes predictions. Mitigation: (1) Data curation - balancing representation, removing stereotypical images. (2) Debiasing techniques - adversarial debiasing (prevent encoding protected attributes), re-weighting losses to balance groups. (3) Prompt engineering - carefully designed prompts to reduce bias in generation. (4) Post-hoc filtering - detecting and filtering biased outputs. (5) Transparency - documenting model limitations, known biases. (6) Human oversight - human-in-the-loop for high-stakes decisions. No perfect solution - ongoing research and societal discussion needed.",

    "Safety, Privacy, and Misuse: Safety concerns: (1) Misinformation - generating fake but realistic images/videos. (2) Harmful content generation - violence, hate symbols despite content filters. (3) Privacy violations - recognizing individuals from images, exposing private information. (4) Dual use - technologies for good (accessibility) also enable harm (surveillance). (5) Automated decision-making - biased models in hiring, lending, criminal justice. (6) Economic displacement - impact on artists, photographers, content creators. Approaches: (1) Safety classifiers filtering harmful inputs/outputs. (2) Watermarking generated content. (3) Privacy-preserving techniques (differential privacy, federated learning). (4) Usage policies and terms of service. (5) Regulatory frameworks - laws governing AI use. (6) Stakeholder engagement - involving affected communities in development. (7) Red-teaming - proactively testing for failures. (8) Interpretability - understanding model decisions for accountability. Responsible AI development requires technical and social solutions."
], img)

# SLIDE 50: Future Directions
log_status("Future Directions - research frontiers")
img = create_simple_diagram("Future", ["Larger\nModels", "More\nModalities", "Better\nReasoning", "Embodied\nAI"])
add_comprehensive_slide("Future Directions and Open Problems", [
    "Scaling: Models, Data, Compute: Trend toward larger models - from CLIP's 400M parameters to Flamingo's 80B to potentially trillion-parameter multimodal models. Scaling laws suggest performance improves log-linearly with compute. Data scaling: from millions to billions of training pairs. Emerging questions: Does scaling plateau? How to scale efficiently? Alternative architectures (Mixture of Experts, state space models) for better scaling. Compute efficiency - reducing cost per training run. Environmental impact of large-scale training. Democratization - making powerful models accessible beyond well-funded labs through parameter-efficient methods, distillation, quantization. Open-source alternatives to proprietary frontier models.",

    "Deeper Reasoning and Understanding: Current models excel at pattern matching but struggle with: (1) Causal reasoning - understanding why, not just what. (2) Counterfactual reasoning - 'what if' scenarios. (3) Abstract reasoning - analogies, logic puzzles. (4) Compositional generalization - systematic generalization to novel combinations. (5) Temporal reasoning in long videos. (6) Multi-step logical inference. (7) Physical understanding - intuitive physics, 3D geometry. Promising directions: (1) Neuro-symbolic AI - combining neural networks with symbolic reasoning. (2) Program synthesis - generating executable programs for reasoning. (3) External memory and retrieval - augmenting models with knowledge bases. (4) Chain-of-thought and prompt engineering. (5) Reinforcement learning from human feedback (RLHF) for alignment. (6) Curriculum learning - training on progressively harder reasoning tasks.",

    "Expanded Modalities and Embodiment: Beyond vision and language: (1) Audio and speech - richer audio understanding beyond transcription. (2) 3D vision - understanding 3D geometry, spatial relationships, scene reconstruction. (3) Haptics and touch - for robotics. (4) Proprioception - body pose and movement awareness. (5) Chemical sensors - smell, taste. (6) Brain signals - EEG, fMRI for brain-computer interfaces. (7) Genomics and proteomics - molecular modalities. Embodied AI: grounding language and vision in physical interaction. Robotics - vision-language models for manipulation, navigation. Virtual environments - training in simulation for real-world transfer. Active perception - models that ask questions, seek information, interact with environment rather than passive observation.",

    "Interaction, Personalization, and Continual Learning: From static models to interactive agents: (1) Dialog - multi-turn conversation about visual content. (2) Clarification - asking for clarification when uncertain. (3) Feedback - learning from user corrections. (4) Personalization - adapting to individual users' preferences, communication styles. (5) Continual learning - learning new concepts without forgetting old ones, adapting to distribution shift over time. (6) Few-shot adaptation - quickly learning from few examples. (7) Multi-agent collaboration - multiple models working together. Technical challenges: (1) Catastrophic forgetting when updating models. (2) Efficient online learning. (3) Privacy-preserving personalization. (4) Evaluation of long-term interactive systems. Open questions: How to safely update deployed models? How to align with diverse human preferences? How to ensure models improve over time while remaining safe?"
], img)

print("\n✓ Slides 37-50 completed with comprehensive content")
print("✓ STATUS: 50/78 slides completed (64% done!)")
print("✓ Continuing with slides 51-78 (Implementation, Conclusion)...\n")


# Save the preliminary presentation with slides 1-50
output_file = '/home/user/experiments/Multimodal_LLM_Lecture_PRELIMINARY_50_Slides.pptx'
prs.save(output_file)
print(f"\n{'='*80}")
print(f"✓ PRELIMINARY PRESENTATION SAVED: {output_file}")
print(f"✓ Contains 50 slides with comprehensive theoretical content")
print(f"✓ Each slide has 3-5 detailed paragraphs")
print(f"{'='*80}\n")
