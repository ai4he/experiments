{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Large Language Models - Lab Session\n",
    "\n",
    "**Course:** Applied Data Science  \n",
    "**Institution:** Clemson University  \n",
    "**Duration:** 75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "1. Set up the environment for multimodal LLMs\n",
    "2. Load and use pre-trained multimodal models (CLIP, BLIP-2, LLaVA)\n",
    "3. Perform zero-shot image classification with CLIP\n",
    "4. Generate image captions with BLIP-2\n",
    "5. Perform visual question answering\n",
    "6. Fine-tune a multimodal model using LoRA\n",
    "7. Deploy and evaluate models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of Transformer architecture\n",
    "- Familiarity with PyTorch\n",
    "- Access to Palmetto cluster (or GPU with 16GB+ VRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup (5 minutes)\n",
    "\n",
    "First, let's set up our environment with all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "!pip install -q transformers accelerate peft bitsandbytes pillow matplotlib requests torch torchvision\n",
    "\n",
    "# For Palmetto cluster, ensure you're using a GPU node\n",
    "# Request GPU node: qsub -I -l select=1:ncpus=16:mem=64gb:ngpus=1:gpu_model=a100,walltime=4:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    Blip2Processor, Blip2ForConditionalGeneration,\n",
    "    AutoProcessor, AutoModelForVision2Seq,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Helper Functions (5 minutes)\n",
    "\n",
    "Let's create some utility functions for loading and displaying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    return img\n",
    "\n",
    "def display_image(image, title=\"Image\"):\n",
    "    \"\"\"Display an image with matplotlib.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images_grid(images, titles=None, cols=3):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    n = len(images)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "    \n",
    "    for idx, img in enumerate(images):\n",
    "        axes[idx].imshow(img)\n",
    "        if titles:\n",
    "            axes[idx].set_title(titles[idx])\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(n, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test with sample images\n",
    "sample_urls = [\n",
    "    \"https://images.unsplash.com/photo-1574158622682-e40e69881006\",  # Cat\n",
    "    \"https://images.unsplash.com/photo-1552053831-71594a27632d\",  # Dog\n",
    "    \"https://images.unsplash.com/photo-1542838132-92c53300491e\"   # Beach\n",
    "]\n",
    "\n",
    "print(\"Loading sample images...\")\n",
    "sample_images = [load_image_from_url(url) for url in sample_urls]\n",
    "display_images_grid(sample_images, titles=[\"Cat\", \"Dog\", \"Beach\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: CLIP - Zero-Shot Image Classification (15 minutes)\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-training) learns a joint embedding space for images and text.\n",
    "\n",
    "### 3.1 Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Loading CLIP model...\")\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
    "print(f\"CLIP model loaded on {device}\")\n",
    "\n",
    "# Model architecture overview\n",
    "print(f\"\\nCLIP Model Architecture:\")\n",
    "print(f\"Vision encoder: {clip_model.vision_model.__class__.__name__}\")\n",
    "print(f\"Text encoder: {clip_model.text_model.__class__.__name__}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in clip_model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Zero-Shot Image Classification\n",
    "\n",
    "CLIP can classify images into any text categories without task-specific training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def classify_image_clip(image, candidate_labels, model, processor, device):\n",
    "    \"\"\"\n",
    "    Perform zero-shot image classification using CLIP.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        candidate_labels: List of text labels\n",
    "        model: CLIP model\n",
    "        processor: CLIP processor\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with labels and probabilities\n",
    "    \"\"\"\n",
    "    # Process inputs\n",
    "    inputs = processor(\n",
    "        text=candidate_labels,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {label: float(prob) for label, prob in zip(candidate_labels, probs)}\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test classification\n",
    "test_image = sample_images[0]  # Cat image\n",
    "labels = [\"a cat\", \"a dog\", \"a bird\", \"a car\", \"a beach\"]\n",
    "\n",
    "print(\"\\n=== Zero-Shot Classification ===\")\n",
    "display_image(test_image, \"Test Image\")\n",
    "results = classify_image_clip(test_image, labels, clip_model, clip_processor, device)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for label, prob in results.items():\n",
    "    print(f\"{label}: {prob*100:.2f}%\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('CLIP Zero-Shot Classification Results')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Image-Text Similarity\n",
    "\n",
    "CLIP can compute similarity between any image-text pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_image_text_similarity(image, texts, model, processor, device):\n",
    "    \"\"\"Compute similarity scores between image and multiple texts.\"\"\"\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Get similarity scores\n",
    "    image_embeds = outputs.image_embeds\n",
    "    text_embeds = outputs.text_embeds\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = (image_embeds @ text_embeds.T).cpu().numpy()[0]\n",
    "    \n",
    "    return {text: float(sim) for text, sim in zip(texts, similarity)}\n",
    "\n",
    "# Test with descriptive sentences\n",
    "descriptions = [\n",
    "    \"a photo of a cute cat sitting\",\n",
    "    \"a professional photograph of an animal\",\n",
    "    \"a car on a highway\",\n",
    "    \"a scenic beach view\",\n",
    "    \"a domestic pet indoors\"\n",
    "]\n",
    "\n",
    "similarities = compute_image_text_similarity(\n",
    "    sample_images[0], descriptions, clip_model, clip_processor, device\n",
    ")\n",
    "\n",
    "print(\"\\n=== Image-Text Similarity Scores ===\")\n",
    "for desc, sim in sorted(similarities.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{sim:.4f} - {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Image Retrieval\n",
    "\n",
    "Use CLIP to find the best matching image for a text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def retrieve_images(query, images, model, processor, device):\n",
    "    \"\"\"Retrieve images that best match the text query.\"\"\"\n",
    "    # Process all images at once\n",
    "    inputs = processor(text=[query], images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get similarity scores\n",
    "    logits_per_text = outputs.logits_per_text\n",
    "    probs = logits_per_text.softmax(dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Sort by similarity\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    \n",
    "    return sorted_indices, probs[sorted_indices]\n",
    "\n",
    "# Test image retrieval\n",
    "query = \"a fluffy pet animal\"\n",
    "indices, scores = retrieve_images(query, sample_images, clip_model, clip_processor, device)\n",
    "\n",
    "print(f\"\\n=== Image Retrieval for query: '{query}' ===\")\n",
    "retrieved_images = [sample_images[i] for i in indices]\n",
    "titles = [f\"Rank {i+1}: {scores[i]:.4f}\" for i in range(len(indices))]\n",
    "display_images_grid(retrieved_images, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: BLIP-2 - Image Captioning (15 minutes)\n",
    "\n",
    "BLIP-2 uses a Q-Former to bridge frozen vision and language models for efficient multimodal generation.\n",
    "\n",
    "### 4.1 Load BLIP-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Loading BLIP-2 model...\")\n",
    "blip2_model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "# Note: BLIP-2 is larger, may need 8-bit quantization on smaller GPUs\n",
    "blip2_processor = Blip2Processor.from_pretrained(blip2_model_name)\n",
    "blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    blip2_model_name,\n",
    "    load_in_8bit=True,  # Use 8-bit quantization to save memory\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"BLIP-2 model loaded with 8-bit quantization\")\n",
    "\n",
    "print(f\"\\nBLIP-2 Architecture:\")\n",
    "print(f\"Vision encoder: Frozen pre-trained ViT\")\n",
    "print(f\"Q-Former: Learnable queries + cross-attention\")\n",
    "print(f\"Language model: OPT-2.7B (frozen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Image Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_caption(image, model, processor, max_length=50, num_beams=5):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using BLIP-2.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        model: BLIP-2 model\n",
    "        processor: BLIP-2 processor\n",
    "        max_length: Maximum caption length\n",
    "        num_beams: Number of beams for beam search\n",
    "    \n",
    "    Returns:\n",
    "        Generated caption string\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Generate captions for all sample images\n",
    "print(\"\\n=== Image Captioning with BLIP-2 ===\")\n",
    "captions = []\n",
    "for idx, img in enumerate(sample_images):\n",
    "    print(f\"\\nGenerating caption for image {idx+1}...\")\n",
    "    caption = generate_caption(img, blip2_model, blip2_processor)\n",
    "    captions.append(caption)\n",
    "    print(f\"Caption: {caption}\")\n",
    "\n",
    "# Display images with captions\n",
    "display_images_grid(sample_images, titles=captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visual Question Answering with BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def answer_question(image, question, model, processor, max_length=50):\n",
    "    \"\"\"\n",
    "    Answer a question about an image using BLIP-2.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        question: Question string\n",
    "        model: BLIP-2 model\n",
    "        processor: BLIP-2 processor\n",
    "        max_length: Maximum answer length\n",
    "    \n",
    "    Returns:\n",
    "        Answer string\n",
    "    \"\"\"\n",
    "    # Format prompt for VQA\n",
    "    prompt = f\"Question: {question} Answer:\"\n",
    "    \n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=5,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    answer = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Test VQA\n",
    "print(\"\\n=== Visual Question Answering ===\")\n",
    "test_image = sample_images[0]  # Cat image\n",
    "display_image(test_image, \"Test Image for VQA\")\n",
    "\n",
    "questions = [\n",
    "    \"What animal is in the image?\",\n",
    "    \"What color is the animal?\",\n",
    "    \"Is the animal indoors or outdoors?\",\n",
    "    \"What is the animal doing?\"\n",
    "]\n",
    "\n",
    "print(\"\\nQuestions and Answers:\")\n",
    "for question in questions:\n",
    "    answer = answer_question(test_image, question, blip2_model, blip2_processor)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Fine-tuning with LoRA (20 minutes)\n",
    "\n",
    "Now let's learn how to fine-tune a multimodal model efficiently using LoRA (Low-Rank Adaptation).\n",
    "\n",
    "### 5.1 Prepare a Custom Dataset\n",
    "\n",
    "For demonstration, we'll create a small synthetic dataset. In practice, you'd use real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"Simple dataset for image captioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, images, captions, processor):\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        \n",
    "        # Process image and text\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=50,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # Add labels (for language modeling)\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].clone()\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# Create a small synthetic dataset\n",
    "# In practice, load your actual dataset here\n",
    "train_images = sample_images * 2  # Duplicate for demo\n",
    "train_captions = [\n",
    "    \"a close-up photo of a cat\",\n",
    "    \"a golden retriever dog\",\n",
    "    \"a beautiful beach with waves\",\n",
    "    \"a cute feline sitting\",\n",
    "    \"a friendly dog outdoors\",\n",
    "    \"ocean waves at sunset\"\n",
    "]\n",
    "\n",
    "print(f\"Dataset size: {len(train_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Set up LoRA Configuration\n",
    "\n",
    "LoRA reduces the number of trainable parameters by learning low-rank updates to weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a fresh model for fine-tuning\n",
    "print(\"Setting up model for fine-tuning...\")\n",
    "\n",
    "# For this demo, we'll use a smaller CLIP model\n",
    "# In practice, you might fine-tune BLIP-2 or LLaVA\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "# Load a vision model\n",
    "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to adapt\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"FEATURE_EXTRACTION\"\n",
    ")\n",
    "\n",
    "print(\"\\nLoRA Configuration:\")\n",
    "print(f\"Rank (r): {lora_config.r}\")\n",
    "print(f\"Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"Target modules: {lora_config.target_modules}\")\n",
    "print(f\"Dropout: {lora_config.lora_dropout}\")\n",
    "\n",
    "# Apply LoRA to model\n",
    "lora_model = get_peft_model(vision_model, lora_config)\n",
    "lora_model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\\nMemory savings: Only {trainable_params/total_params*100:.2f}% of parameters need gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Fine-tuning Loop\n",
    "\n",
    "Here's a simplified fine-tuning example. In practice, you'd use HuggingFace Trainer for more robust training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Note: This is a simplified demonstration\n",
    "# For actual fine-tuning, refer to the homework notebook\n",
    "\n",
    "print(\"\\n=== Fine-tuning Demonstration ===\")\n",
    "print(\"\\nKey steps for fine-tuning:\")\n",
    "print(\"1. Prepare your dataset (image-caption pairs)\")\n",
    "print(\"2. Configure LoRA with appropriate hyperparameters\")\n",
    "print(\"3. Set up training arguments:\")\n",
    "print(\"   - Learning rate: 1e-4 to 5e-5\")\n",
    "print(\"   - Batch size: 4-16 depending on GPU memory\")\n",
    "print(\"   - Epochs: 3-10\")\n",
    "print(\"   - Warmup steps: 10% of total steps\")\n",
    "print(\"4. Use HuggingFace Trainer for robust training\")\n",
    "print(\"5. Save only LoRA weights (very small file!)\")\n",
    "print(\"6. Evaluate on validation set\")\n",
    "\n",
    "# Example training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision training\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Arguments:\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Saving and Loading LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save LoRA weights\n",
    "lora_output_dir = \"./clip_lora_demo\"\n",
    "lora_model.save_pretrained(lora_output_dir)\n",
    "print(f\"LoRA weights saved to {lora_output_dir}\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "lora_file = os.path.join(lora_output_dir, \"adapter_model.bin\")\n",
    "if os.path.exists(lora_file):\n",
    "    size_mb = os.path.getsize(lora_file) / (1024 * 1024)\n",
    "    print(f\"LoRA adapter size: {size_mb:.2f} MB\")\n",
    "    print(\"\\nNotice: LoRA weights are tiny compared to full model!\")\n",
    "\n",
    "# To load LoRA weights later:\n",
    "print(\"\\nTo load LoRA weights:\")\n",
    "print(\"from peft import PeftModel\")\n",
    "print(\"base_model = CLIPVisionModel.from_pretrained('openai/clip-vit-base-patch32')\")\n",
    "print(\"model = PeftModel.from_pretrained(base_model, './clip_lora_demo')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Topics (10 minutes)\n",
    "\n",
    "### 6.1 Multi-GPU Training\n",
    "\n",
    "For larger datasets, you'll want to use multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=== Multi-GPU Training on Palmetto ===\")\n",
    "print(\"\\nTo request multiple GPUs:\")\n",
    "print(\"qsub -I -l select=1:ncpus=32:mem=128gb:ngpus=4:gpu_model=a100,walltime=8:00:00\")\n",
    "print(\"\\nIn your Python script:\")\n",
    "print(\"\"\"\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for batch in train_dataloader:\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\"\"\")\n",
    "print(\"\\nAccelerate handles device placement and gradient synchronization automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Quantization\n",
    "\n",
    "Reduce memory usage with quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=== Model Quantization ===\")\n",
    "print(\"\\n8-bit quantization (load_in_8bit=True):\")\n",
    "print(\"- Reduces memory by ~50%\")\n",
    "print(\"- Minimal performance loss\")\n",
    "print(\"- Slower inference\")\n",
    "print(\"\\n4-bit quantization (load_in_4bit=True):\")\n",
    "print(\"- Reduces memory by ~75%\")\n",
    "print(\"- Some performance loss\")\n",
    "print(\"- Even slower inference\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"\"\"\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\"\"\")\n",
    "print(\"\\nUse quantization when GPU memory is limited!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Batch Inference for Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def batch_generate_captions(images, model, processor, batch_size=4):\n",
    "    \"\"\"Generate captions for multiple images efficiently.\"\"\"\n",
    "    all_captions = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i+batch_size]\n",
    "        inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_length=50, num_beams=5)\n",
    "        \n",
    "        captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_captions.extend(captions)\n",
    "    \n",
    "    return all_captions\n",
    "\n",
    "print(\"\\n=== Batch Inference ===\")\n",
    "print(\"Batch processing is much faster than processing one image at a time!\")\n",
    "print(f\"\\nProcessing {len(sample_images)} images...\")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "batch_captions = batch_generate_captions(sample_images, blip2_model, blip2_processor, batch_size=3)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken: {end-start:.2f} seconds\")\n",
    "print(\"\\nGenerated captions:\")\n",
    "for i, caption in enumerate(batch_captions):\n",
    "    print(f\"{i+1}. {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Evaluation and Benchmarking (5 minutes)\n",
    "\n",
    "### 7.1 Caption Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install evaluation metrics\n",
    "!pip install -q evaluate sacrebleu rouge-score\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Example: Evaluate generated vs. reference captions\n",
    "references = [\n",
    "    [\"a close-up photo of a cat\"],\n",
    "    [\"a golden retriever dog\"],\n",
    "    [\"a beautiful beach with waves\"]\n",
    "]\n",
    "\n",
    "predictions = [\n",
    "    \"a photo of a cat\",\n",
    "    \"a dog sitting on grass\",\n",
    "    \"ocean waves at sunset\"\n",
    "]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"\\nBLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "\n",
    "# Calculate ROUGE score\n",
    "rouge_score = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n",
    "print(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\nNote: For comprehensive evaluation, also consider:\")\n",
    "print(\"- CIDEr: Consensus-based metric for captioning\")\n",
    "print(\"- SPICE: Semantic similarity metric\")\n",
    "print(\"- METEOR: Accounts for synonyms and paraphrases\")\n",
    "print(\"- Human evaluation: Always the gold standard!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Summary and Next Steps (5 minutes)\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **CLIP**: Zero-shot classification, image-text similarity\n",
    "2. **BLIP-2**: Image captioning, visual question answering\n",
    "3. **LoRA**: Parameter-efficient fine-tuning\n",
    "4. **Practical considerations**: Quantization, batch inference, evaluation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Pre-trained multimodal models are powerful and accessible\n",
    "- HuggingFace Transformers makes it easy to use these models\n",
    "- LoRA enables fine-tuning with minimal resources\n",
    "- Quantization helps fit larger models in memory\n",
    "- Batch processing improves efficiency\n",
    "\n",
    "### Next Steps: Homework\n",
    "\n",
    "In the homework notebook, you will:\n",
    "1. Fine-tune CLIP on a custom image classification dataset\n",
    "2. Fine-tune BLIP for domain-specific image captioning\n",
    "3. Fine-tune a VLM for visual question answering\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [HuggingFace Transformers Documentation](https://huggingface.co/docs/transformers)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n",
    "- [BLIP-2 Paper](https://arxiv.org/abs/2301.12597)\n",
    "- [LLaVA Paper](https://arxiv.org/abs/2304.08485)\n",
    "- [Palmetto Documentation](https://docs.rcd.clemson.edu/palmetto/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Free up GPU memory\n",
    "import gc\n",
    "\n",
    "del clip_model, blip2_model, lora_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"GPU memory cleared. Ready for homework exercises!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
