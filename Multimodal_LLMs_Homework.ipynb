{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework \u00b7 Multimodal Large Language Models\n",
        "**Course:** Applied Data Science (CPSC 8xxx)\n",
        "**Due:** _One week after the lab session_\n",
        "**Submission:** Push to your private GitLab repository and submit a Palmetto job report.\n",
        "\n",
        "This homework builds on the lecture and lab notebook. You will fine-tune multimodal foundation models across three modality pairings. Each exercise must include:\n",
        "- SLURM script or `srun` command used on Palmetto.\n",
        "- Training/evaluation logs (TensorBoard, W&B, or MLflow).\n",
        "- Short written summary (1\u20132 paragraphs) interpreting results and challenges.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Environment Checklist\n",
        "- Use the same `multimodal-llm` Conda environment from the lab notebook.\n",
        "- Reserve GPUs with at least 24 GB memory (A100 preferred).\n",
        "- Store intermediate checkpoints under `/scratch1/$USER/hw3-multimodal`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 1 \u00b7 Vision-Language Fine-Tuning (20 pts)\n",
        "**Goal:** Fine-tune the CLIP-style dual encoder on the [Clemson Campus Scenes](https://example.org) dataset and evaluate zero-shot transfer to COCO.\n",
        "\n",
        "**Requirements:**\n",
        "- Implement balanced sampling to mitigate class imbalance between campus landmarks.\n",
        "- Apply parameter-efficient fine-tuning (LoRA on the projector or QLoRA on the text tower).\n",
        "- Report Recall@1/5/10 on COCO validation and Clemson Campus validation splits.\n",
        "- Analyze modality alignment drift compared to the pretraining checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: configure paths\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "DATA_ROOT = Path('/scratch1') / os.environ.get('USER', 'student') / 'hw3-multimodal'\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# TODO: implement dataset loader and balanced sampling strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: load pretrained encoders and attach LoRA adapters\n",
        "from transformers import AutoModel\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "vision_encoder = AutoModel.from_pretrained('openai/clip-vit-base-patch16')\n",
        "text_encoder = AutoModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=['query', 'value'])\n",
        "vision_encoder = get_peft_model(vision_encoder, lora_config)\n",
        "text_encoder = get_peft_model(text_encoder, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: training loop\n",
        "# Use Accelerate or PyTorch Lightning if preferred\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: compute retrieval metrics and save a JSON report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 2 \u00b7 Audio-Text Instruction Alignment (25 pts)\n",
        "**Goal:** Adapt Whisper-small + LLaMA-2-7B-chat to follow spoken instructions and output textual answers.\n",
        "\n",
        "**Dataset Suggestions:** AudioCaps, Spoken-SQuAD, campus tour recordings. Combine with synthetic speech generated via Torchaudio + TTS for augmentation.\n",
        "\n",
        "**Requirements:**\n",
        "- Implement a projection layer from Whisper encoder embeddings to the LLaMA hidden size.\n",
        "- Apply LoRA to the language model _or_ freezing strategy plus adapter.\n",
        "- Evaluate on a held-out set with WER and instruction-following accuracy (exact match or ROUGE-L).\n",
        "- Discuss robustness to noisy backgrounds recorded on campus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: prepare audio dataset manifest and dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: build audio-to-text adapter and fine-tuning loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: evaluation metrics (WER, instruction accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 3 \u00b7 Any-to-Any Generation (35 pts)\n",
        "**Goal:** Extend an open-source multimodal assistant (e.g., LLaVA-Next, InstructBLIP, or Qwen-VL) to support **chart-to-audio** and **audio-to-image** tasks via tool augmentation.\n",
        "\n",
        "**Requirements:**\n",
        "- Add at least two modality adapters (e.g., Chart OCR encoder + audio synthesizer).\n",
        "- Implement routing logic that selects the correct adapter based on the prompt.\n",
        "- Demonstrate two end-to-end examples per new capability.\n",
        "- Benchmark against a baseline without the new adapters.\n",
        "- Provide an ablation table analyzing latency and GPU memory usage under Palmetto scheduling constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: design routing policy for any-to-any interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: integrate tool calls (e.g., image generation API, TTS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Deliverables Checklist\n",
        "- [ ] Completed code cells for all exercises.\n",
        "- [ ] `README.md` in your repository describing data sources and ethical considerations.\n",
        "- [ ] SLURM submission scripts (`*.sbatch`).\n",
        "- [ ] Evaluation summaries in `reports/`.\n",
        "- [ ] Short reflection (submit via Canvas) covering lessons learned and next steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Grading Rubric\n",
        "| Component | Points | Criteria |\n",
        "| --- | --- | --- |\n",
        "| Exercise 1 | 20 | Completeness, retrieval metrics, analysis |\n",
        "| Exercise 2 | 25 | Adapter design, instruction accuracy, robustness discussion |\n",
        "| Exercise 3 | 35 | Tool integration, demonstrations, ablation |\n",
        "| Engineering Report | 10 | Clarity of SLURM scripts, logging, reproducibility |\n",
        "| Responsible AI Reflection | 10 | Bias/safety analysis, mitigation proposals |\n",
        "\n",
        "Late policy: -10% per day (max 3 days). Collaborations must be declared.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}