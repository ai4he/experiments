{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Large Language Models - Homework Assignment\n",
    "\n",
    "**Course:** Applied Data Science  \n",
    "**Institution:** Clemson University  \n",
    "**Due Date:** [To be specified by instructor]\n",
    "\n",
    "## Overview\n",
    "\n",
    "This homework consists of three exercises that will give you hands-on experience fine-tuning multimodal models for different tasks and modalities. Each exercise focuses on a different aspect of multimodal learning:\n",
    "\n",
    "1. **Exercise 1**: Fine-tune CLIP for custom image classification (Vision-Language Contrastive Learning)\n",
    "2. **Exercise 2**: Fine-tune BLIP for domain-specific image captioning (Vision-to-Language Generation)\n",
    "3. **Exercise 3**: Fine-tune a VLM for Visual Question Answering (Multimodal Understanding)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Prepare datasets for multimodal fine-tuning\n",
    "- Apply parameter-efficient fine-tuning techniques (LoRA)\n",
    "- Train models on Palmetto cluster\n",
    "- Evaluate multimodal models quantitatively\n",
    "- Compare different modality fusion strategies\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "1. Completed Jupyter notebook with all code cells executed\n",
    "2. Written analysis for each exercise (in markdown cells)\n",
    "3. Model checkpoints (LoRA weights only)\n",
    "4. Evaluation results and visualizations\n",
    "5. Brief report (1-2 pages) summarizing findings\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "- **Exercise 1**: 30 points\n",
    "- **Exercise 2**: 30 points\n",
    "- **Exercise 3**: 30 points\n",
    "- **Analysis and Report**: 10 points\n",
    "- **Total**: 100 points\n",
    "\n",
    "## Setup\n",
    "\n",
    "Ensure you have access to Palmetto cluster with GPU resources. Recommended configuration:\n",
    "- GPU: A100 (40GB) or V100 (32GB)\n",
    "- Memory: 64GB+ RAM\n",
    "- Storage: 50GB+ in /scratch\n",
    "\n",
    "Request GPU node:\n",
    "```bash\n",
    "qsub -I -l select=1:ncpus=16:mem=64gb:ngpus=1:gpu_model=a100,walltime=8:00:00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "Run this section once to set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate peft bitsandbytes datasets pillow matplotlib \\\n",
    "    scikit-learn evaluate sacrebleu rouge-score torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    Blip2Processor, Blip2ForConditionalGeneration,\n",
    "    AutoProcessor, AutoModelForVision2Seq,\n",
    "    TrainingArguments, Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Fine-tune CLIP for Custom Image Classification (30 points)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Fine-tune CLIP on a domain-specific dataset to improve zero-shot classification performance. You will use the **Food-101** dataset to adapt CLIP for food recognition.\n",
    "\n",
    "## Background\n",
    "\n",
    "CLIP is trained on general web data. For specialized domains, fine-tuning can significantly improve performance. You'll use contrastive learning with LoRA to adapt CLIP efficiently.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Load and prepare the Food-101 dataset (use a subset for faster training)\n",
    "2. Configure LoRA for CLIP's vision and text encoders\n",
    "3. Implement contrastive loss training\n",
    "4. Fine-tune the model\n",
    "5. Evaluate zero-shot classification performance\n",
    "6. Compare with baseline (non-fine-tuned) CLIP\n",
    "\n",
    "## Grading Criteria\n",
    "\n",
    "- Data preparation (5 points)\n",
    "- LoRA configuration (5 points)\n",
    "- Training implementation (10 points)\n",
    "- Evaluation and comparison (5 points)\n",
    "- Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Food-101 dataset (subset for faster training)\n",
    "# Full dataset: 101 food categories, 101,000 images\n",
    "# We'll use a subset: 20 categories, ~20,000 images\n",
    "\n",
    "print(\"Loading Food-101 dataset...\")\n",
    "dataset = load_dataset(\"food101\", split=\"train[:20%]\")  # Use 20% of training data\n",
    "test_dataset = load_dataset(\"food101\", split=\"validation[:20%]\")\n",
    "\n",
    "print(f\"Training samples: {len(dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"\\nDataset features:\", dataset.features)\n",
    "print(f\"Number of classes: {len(dataset.features['label'].names)}\")\n",
    "print(f\"Class names (first 10): {dataset.features['label'].names[:10]}\")\n",
    "\n",
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    sample = dataset[idx * 200]\n",
    "    ax.imshow(sample['image'])\n",
    "    ax.set_title(dataset.features['label'].names[sample['label']])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Split dataset into train/validation\n",
    "# Hint: Use dataset.train_test_split()\n",
    "train_val_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n",
    "\n",
    "print(f\"\\nFinal split:\")\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Validation: {len(val_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare CLIP Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Get class names and create text prompts\n",
    "class_names = dataset.features['label'].names\n",
    "text_prompts = [f\"a photo of {name.replace('_', ' ')}\" for name in class_names]\n",
    "\n",
    "print(f\"\\nText prompts (first 5):\")\n",
    "for i in range(5):\n",
    "    print(f\"  {i}: {text_prompts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evaluate Baseline Performance\n",
    "\n",
    "First, evaluate the pre-trained CLIP model to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_clip_classification(model, processor, dataset, text_prompts, device, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate CLIP model on classification task.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Classification accuracy\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"Evaluating on {len(dataset)} samples...\")\n",
    "    \n",
    "    # TODO: Implement evaluation loop\n",
    "    # For each image:\n",
    "    #   1. Process image and text prompts\n",
    "    #   2. Get model predictions\n",
    "    #   3. Compare with ground truth label\n",
    "    #   4. Update accuracy\n",
    "    \n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image']\n",
    "        true_label = sample['label']\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(\n",
    "            text=text_prompts,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            predicted_label = logits_per_image.argmax(dim=1).item()\n",
    "        \n",
    "        if predicted_label == true_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate baseline\n",
    "print(\"\\n=== Baseline CLIP Evaluation ===\")\n",
    "baseline_accuracy = evaluate_clip_classification(\n",
    "    model, processor, test_dataset, text_prompts, device, num_samples=500\n",
    ")\n",
    "print(f\"\\nBaseline Accuracy: {baseline_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configure LoRA for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Configure LoRA for CLIP\n",
    "# Apply LoRA to both vision and text encoders\n",
    "# Recommended hyperparameters:\n",
    "#   - r: 8-16\n",
    "#   - lora_alpha: 16-32\n",
    "#   - target_modules: [\"q_proj\", \"v_proj\"] for both encoders\n",
    "#   - lora_dropout: 0.1\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create Dataset Class for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CLIPContrastiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for CLIP contrastive learning.\n",
    "    Returns image-text pairs for each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, text_prompts, processor):\n",
    "        self.dataset = dataset\n",
    "        self.text_prompts = text_prompts\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image']\n",
    "        label = sample['label']\n",
    "        text = self.text_prompts[label]\n",
    "        \n",
    "        # Process image and text\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding['labels'] = label\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# Create datasets\n",
    "train_clip_dataset = CLIPContrastiveDataset(train_dataset, text_prompts, processor)\n",
    "val_clip_dataset = CLIPContrastiveDataset(val_dataset, text_prompts, processor)\n",
    "\n",
    "print(f\"Training dataset: {len(train_clip_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_clip_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Define Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Define training arguments\n",
    "# Recommended settings:\n",
    "#   - learning_rate: 5e-5 to 1e-4\n",
    "#   - num_train_epochs: 3-5\n",
    "#   - per_device_train_batch_size: 16-32 (adjust based on GPU memory)\n",
    "#   - warmup_steps: 500\n",
    "#   - logging_steps: 50\n",
    "#   - save_steps: 500\n",
    "#   - evaluation_strategy: \"steps\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./clip_food101_lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=2,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "print(f\"Total training steps: {len(train_clip_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Custom Trainer with Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CLIPContrastiveTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom trainer for CLIP with contrastive loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Compute contrastive loss for CLIP.\n",
    "        \"\"\"\n",
    "        # TODO: Implement contrastive loss computation\n",
    "        # 1. Forward pass through model\n",
    "        # 2. Get image and text embeddings\n",
    "        # 3. Compute contrastive loss (InfoNCE)\n",
    "        \n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # CLIP returns logits_per_image and logits_per_text\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        \n",
    "        # Create target labels (diagonal should be high)\n",
    "        batch_size = logits_per_image.shape[0]\n",
    "        targets = torch.arange(batch_size).to(logits_per_image.device)\n",
    "        \n",
    "        # Symmetric loss (image-to-text and text-to-image)\n",
    "        loss_i2t = nn.CrossEntropyLoss()(logits_per_image, targets)\n",
    "        loss_t2i = nn.CrossEntropyLoss()(logits_per_text, targets)\n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"Custom trainer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize trainer\n",
    "trainer = CLIPContrastiveTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_clip_dataset,\n",
    "    eval_dataset=val_clip_dataset,\n",
    ")\n",
    "\n",
    "# TODO: Train the model\n",
    "print(\"\\n=== Starting Training ===\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n=== Training Completed ===\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"./clip_food101_final\")\n",
    "print(\"\\nModel saved to ./clip_food101_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate fine-tuned model\n",
    "print(\"\\n=== Fine-tuned CLIP Evaluation ===\")\n",
    "finetuned_accuracy = evaluate_clip_classification(\n",
    "    model, processor, test_dataset, text_prompts, device, num_samples=500\n",
    ")\n",
    "print(f\"\\nFine-tuned Accuracy: {finetuned_accuracy*100:.2f}%\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = (finetuned_accuracy - baseline_accuracy) * 100\n",
    "print(f\"\\n=== Results Comparison ===\")\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy*100:.2f}%\")\n",
    "print(f\"Fine-tuned Accuracy: {finetuned_accuracy*100:.2f}%\")\n",
    "print(f\"Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['Baseline', 'Fine-tuned']\n",
    "accuracies = [baseline_accuracy * 100, finetuned_accuracy * 100]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "plt.bar(models, accuracies, color=colors)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('CLIP Classification Performance')\n",
    "plt.ylim([0, 100])\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Analysis Questions\n",
    "\n",
    "**TODO: Answer the following questions in the markdown cell below:**\n",
    "\n",
    "1. What was the improvement in accuracy after fine-tuning? Why do you think this happened?\n",
    "2. How many parameters did LoRA add compared to the full model? What are the benefits?\n",
    "3. What challenges did you face during training? How did you address them?\n",
    "4. How would you improve the results further?\n",
    "5. In what real-world scenarios would this fine-tuned model be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis:**\n",
    "\n",
    "[Write your analysis here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2: Fine-tune BLIP for Domain-Specific Image Captioning (30 points)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Fine-tune BLIP-2 for generating detailed captions in a specialized domain. You will use a medical imaging dataset to adapt BLIP for radiology report generation.\n",
    "\n",
    "## Background\n",
    "\n",
    "BLIP-2 is a powerful image captioning model, but it's trained on general images. Medical images require specific terminology and structured descriptions. You'll fine-tune BLIP-2 to generate medically accurate captions.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Load and prepare a medical imaging dataset (or alternative domain-specific dataset)\n",
    "2. Configure LoRA for BLIP-2's Q-Former and language model\n",
    "3. Implement image captioning fine-tuning\n",
    "4. Train the model\n",
    "5. Evaluate using BLEU, ROUGE, and qualitative analysis\n",
    "6. Generate captions for test images\n",
    "\n",
    "## Grading Criteria\n",
    "\n",
    "- Data preparation (5 points)\n",
    "- LoRA configuration (5 points)\n",
    "- Training implementation (10 points)\n",
    "- Evaluation (5 points)\n",
    "- Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset\n",
    "\n",
    "We'll use the **ROCO (Radiology Objects in COntext)** dataset or **Flickr8k** as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 1: Use Flickr8k (easier to access)\n",
    "# Option 2: Use medical imaging dataset if available\n",
    "\n",
    "# For this exercise, we'll use a subset of COCO captions as demonstration\n",
    "# You can replace with medical imaging dataset\n",
    "\n",
    "print(\"Loading dataset for image captioning...\")\n",
    "\n",
    "# TODO: Load your dataset\n",
    "# For demonstration, using a small caption dataset\n",
    "# Replace with actual medical imaging or domain-specific dataset\n",
    "\n",
    "caption_dataset = load_dataset(\"nlphuji/flickr30k\", split=\"test[:10%]\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(caption_dataset)} samples\")\n",
    "print(f\"Features: {caption_dataset.features}\")\n",
    "\n",
    "# Explore samples\n",
    "sample = caption_dataset[0]\n",
    "print(f\"\\nSample image shape: {sample['image'].size}\")\n",
    "print(f\"Sample captions: {sample['caption'][:2]}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    sample = caption_dataset[idx * 100]\n",
    "    ax.imshow(sample['image'])\n",
    "    caption = sample['caption'][0] if isinstance(sample['caption'], list) else sample['caption']\n",
    "    ax.set_title(caption[:50] + '...', fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare BLIP-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load BLIP-2 model\n",
    "# Using smaller version for faster training\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "blip_processor = Blip2Processor.from_pretrained(model_name)\n",
    "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # Use quantization to save memory\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "blip_model = prepare_model_for_kbit_training(blip_model)\n",
    "print(\"Model prepared for k-bit training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generate Baseline Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_captions_batch(images, model, processor, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate captions for a batch of images.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=5,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return captions\n",
    "\n",
    "# Generate baseline captions for a few samples\n",
    "print(\"\\n=== Baseline Captions ===\")\n",
    "num_samples = 5\n",
    "for i in range(num_samples):\n",
    "    sample = caption_dataset[i * 200]\n",
    "    image = sample['image']\n",
    "    reference_caption = sample['caption'][0] if isinstance(sample['caption'], list) else sample['caption']\n",
    "    \n",
    "    generated_caption = generate_captions_batch([image], blip_model, blip_processor)[0]\n",
    "    \n",
    "    print(f\"\\nImage {i+1}:\")\n",
    "    print(f\"Reference: {reference_caption}\")\n",
    "    print(f\"Generated: {generated_caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Configure LoRA for BLIP-2\n",
    "# Focus on Q-Former and language model\n",
    "# Recommended settings:\n",
    "#   - r: 8-16\n",
    "#   - lora_alpha: 32\n",
    "#   - target_modules: Look at model architecture and select appropriate modules\n",
    "#   - For language model: [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
    "\n",
    "lora_config_blip = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "blip_model = get_peft_model(blip_model, lora_config_blip)\n",
    "blip_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA applied to BLIP-2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image captioning training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, processor, max_length=50):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image']\n",
    "        caption = sample['caption']\n",
    "        \n",
    "        # Handle multiple captions (take first one)\n",
    "        if isinstance(caption, list):\n",
    "            caption = caption[0]\n",
    "        \n",
    "        # TODO: Process image and caption\n",
    "        # Hint: Use processor to encode both image and text\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # Labels for language modeling\n",
    "        encoding['labels'] = encoding['input_ids'].clone()\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# Split dataset\n",
    "train_val_split = caption_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_caption_data = train_val_split['train']\n",
    "val_caption_data = train_val_split['test']\n",
    "\n",
    "# Create datasets\n",
    "train_captioning_dataset = ImageCaptioningDataset(train_caption_data, blip_processor)\n",
    "val_captioning_dataset = ImageCaptioningDataset(val_caption_data, blip_processor)\n",
    "\n",
    "print(f\"Training dataset: {len(train_captioning_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_captioning_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Define training arguments for BLIP-2\n",
    "training_args_blip = TrainingArguments(\n",
    "    output_dir=\"./blip2_captioning_lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize trainer\n",
    "trainer_blip = Trainer(\n",
    "    model=blip_model,\n",
    "    args=training_args_blip,\n",
    "    train_dataset=train_captioning_dataset,\n",
    "    eval_dataset=val_captioning_dataset,\n",
    ")\n",
    "\n",
    "# TODO: Train the model\n",
    "print(\"\\n=== Starting BLIP-2 Fine-tuning ===\")\n",
    "train_result_blip = trainer_blip.train()\n",
    "\n",
    "print(\"\\n=== Training Completed ===\")\n",
    "print(f\"Training loss: {train_result_blip.training_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "trainer_blip.save_model(\"./blip2_captioning_final\")\n",
    "print(\"\\nModel saved to ./blip2_captioning_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_captioning(model, processor, dataset, num_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate captioning model using BLEU and ROUGE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Evaluating on {num_samples} samples...\")\n",
    "    \n",
    "    # TODO: Generate captions and compare with references\n",
    "    for idx in tqdm(range(min(num_samples, len(dataset)))):\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image']\n",
    "        reference = sample['caption']\n",
    "        if isinstance(reference, list):\n",
    "            reference = reference[0]\n",
    "        \n",
    "        # Generate caption\n",
    "        generated = generate_captions_batch([image], model, processor)[0]\n",
    "        \n",
    "        references.append([reference])\n",
    "        predictions.append(generated)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=[r[0] for r in references])\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score['bleu'],\n",
    "        'rouge_l': rouge_score['rougeL'],\n",
    "        'predictions': predictions[:10],  # Save first 10 for analysis\n",
    "        'references': references[:10]\n",
    "    }\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n=== Evaluating Fine-tuned Model ===\")\n",
    "results = evaluate_captioning(blip_model, blip_processor, val_caption_data, num_samples=100)\n",
    "\n",
    "print(f\"\\nBLEU Score: {results['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L Score: {results['rouge_l']:.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n=== Sample Predictions ===\")\n",
    "for i in range(5):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Reference: {results['references'][i][0]}\")\n",
    "    print(f\"Predicted: {results['predictions'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate and visualize captions for test images\n",
    "print(\"\\n=== Qualitative Results ===\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(6):\n",
    "    sample = val_caption_data[idx * 50]\n",
    "    image = sample['image']\n",
    "    reference = sample['caption'][0] if isinstance(sample['caption'], list) else sample['caption']\n",
    "    \n",
    "    # Generate caption\n",
    "    generated = generate_captions_batch([image], blip_model, blip_processor)[0]\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f\"Ref: {reference[:40]}...\\nGen: {generated[:40]}...\", fontsize=9)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Analysis Questions\n",
    "\n",
    "**TODO: Answer the following questions:**\n",
    "\n",
    "1. What were the BLEU and ROUGE scores? How do they compare to baseline?\n",
    "2. Analyze the generated captions: What did the model learn? What mistakes does it still make?\n",
    "3. How suitable is this approach for medical imaging or your chosen domain?\n",
    "4. What data augmentation techniques could improve performance?\n",
    "5. How would you evaluate caption quality beyond automatic metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis:**\n",
    "\n",
    "[Write your analysis here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3: Fine-tune VLM for Visual Question Answering (30 points)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Fine-tune a Vision-Language Model (e.g., BLIP-2 or LLaVA) for visual question answering on a specific domain.\n",
    "\n",
    "## Background\n",
    "\n",
    "VQA requires understanding both visual content and natural language questions to generate accurate answers. You'll fine-tune a model to answer questions about images in a specific domain.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Load and prepare a VQA dataset\n",
    "2. Configure LoRA for the VLM\n",
    "3. Implement VQA fine-tuning\n",
    "4. Train the model\n",
    "5. Evaluate accuracy and answer quality\n",
    "6. Analyze model performance on different question types\n",
    "\n",
    "## Grading Criteria\n",
    "\n",
    "- Data preparation (5 points)\n",
    "- LoRA configuration (5 points)\n",
    "- Training implementation (10 points)\n",
    "- Evaluation (5 points)\n",
    "- Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load VQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load VQA dataset\n",
    "# Options: VQAv2, GQA, OK-VQA, TextVQA\n",
    "# For this exercise, we'll use a subset\n",
    "\n",
    "print(\"Loading VQA dataset...\")\n",
    "\n",
    "# TODO: Load VQA dataset\n",
    "# You can use HuggingFace datasets or download from official sources\n",
    "# For demonstration, we'll create a simple structure\n",
    "\n",
    "# Example: Using a VQA-style dataset\n",
    "try:\n",
    "    vqa_dataset = load_dataset(\"Multimodal-Fatima/OK-VQA_train\", split=\"train[:10%]\")\n",
    "    print(f\"Dataset loaded: {len(vqa_dataset)} samples\")\n",
    "except:\n",
    "    print(\"Could not load OK-VQA. Please use an alternative VQA dataset.\")\n",
    "    # Create dummy data for demonstration\n",
    "    vqa_dataset = None\n",
    "\n",
    "if vqa_dataset:\n",
    "    print(f\"\\nDataset features: {vqa_dataset.features}\")\n",
    "    \n",
    "    # Explore sample\n",
    "    sample = vqa_dataset[0]\n",
    "    print(f\"\\nSample:\")\n",
    "    for key in sample.keys():\n",
    "        if key != 'image':\n",
    "            print(f\"{key}: {sample[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize VQA Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if vqa_dataset:\n",
    "    # Visualize samples\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx in range(6):\n",
    "        sample = vqa_dataset[idx * 100]\n",
    "        image = sample['image']\n",
    "        question = sample.get('question', 'N/A')\n",
    "        answer = sample.get('answer', sample.get('answers', 'N/A'))\n",
    "        \n",
    "        axes[idx].imshow(image)\n",
    "        axes[idx].set_title(f\"Q: {question[:30]}...\\nA: {str(answer)[:30]}...\", fontsize=9)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prepare Model for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Load a VLM suitable for VQA\n",
    "# Options: BLIP-2, InstructBLIP, LLaVA\n",
    "# We'll use BLIP-2 as it's already loaded\n",
    "\n",
    "print(\"Preparing model for VQA fine-tuning...\")\n",
    "\n",
    "# If you've already used blip_model, create a fresh instance\n",
    "vqa_model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "vqa_processor = Blip2Processor.from_pretrained(vqa_model_name)\n",
    "vqa_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    vqa_model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "vqa_model = prepare_model_for_kbit_training(vqa_model)\n",
    "print(\"Model loaded and prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test Baseline VQA Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if vqa_dataset:\n",
    "    def answer_vqa(image, question, model, processor):\n",
    "        \"\"\"Generate answer for a VQA question.\"\"\"\n",
    "        # Format prompt\n",
    "        prompt = f\"Question: {question} Answer:\"\n",
    "        \n",
    "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=20,\n",
    "            num_beams=5\n",
    "        )\n",
    "        \n",
    "        answer = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "    \n",
    "    # Test baseline\n",
    "    print(\"\\n=== Baseline VQA Performance ===\")\n",
    "    for i in range(5):\n",
    "        sample = vqa_dataset[i * 200]\n",
    "        image = sample['image']\n",
    "        question = sample['question']\n",
    "        true_answer = sample.get('answer', sample.get('answers', 'N/A'))\n",
    "        \n",
    "        predicted_answer = answer_vqa(image, question, vqa_model, vqa_processor)\n",
    "        \n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"True Answer: {true_answer}\")\n",
    "        print(f\"Predicted: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Configure LoRA for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Configure LoRA for VQA model\n",
    "lora_config_vqa = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "vqa_model = get_peft_model(vqa_model, lora_config_vqa)\n",
    "vqa_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA configured for VQA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Prepare VQA Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class VQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for VQA training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, processor, max_length=77):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image']\n",
    "        question = sample['question']\n",
    "        answer = sample.get('answer', sample.get('answers', ''))\n",
    "        \n",
    "        # Handle multiple answers (take first or most common)\n",
    "        if isinstance(answer, list):\n",
    "            answer = answer[0]\n",
    "        \n",
    "        # TODO: Format input as \"Question: {question} Answer: {answer}\"\n",
    "        prompt = f\"Question: {question} Answer:\"\n",
    "        \n",
    "        # Encode\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=prompt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Encode answer as labels\n",
    "        answer_encoding = self.processor.tokenizer(\n",
    "            answer,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=20,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding['labels'] = answer_encoding['input_ids'].squeeze(0)\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "if vqa_dataset:\n",
    "    # Split dataset\n",
    "    vqa_split = vqa_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_vqa = vqa_split['train']\n",
    "    val_vqa = vqa_split['test']\n",
    "    \n",
    "    # Create datasets\n",
    "    train_vqa_dataset = VQADataset(train_vqa, vqa_processor)\n",
    "    val_vqa_dataset = VQADataset(val_vqa, vqa_processor)\n",
    "    \n",
    "    print(f\"Training: {len(train_vqa_dataset)} samples\")\n",
    "    print(f\"Validation: {len(val_vqa_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Training Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if vqa_dataset:\n",
    "    # TODO: Define training arguments\n",
    "    training_args_vqa = TrainingArguments(\n",
    "        output_dir=\"./vqa_model_lora\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=100,\n",
    "        logging_steps=50,\n",
    "        save_steps=200,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_accumulation_steps=4,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer_vqa = Trainer(\n",
    "        model=vqa_model,\n",
    "        args=training_args_vqa,\n",
    "        train_dataset=train_vqa_dataset,\n",
    "        eval_dataset=val_vqa_dataset,\n",
    "    )\n",
    "    \n",
    "    # TODO: Train the model\n",
    "    print(\"\\n=== Starting VQA Fine-tuning ===\")\n",
    "    train_result_vqa = trainer_vqa.train()\n",
    "    \n",
    "    print(\"\\n=== Training Completed ===\")\n",
    "    print(f\"Training loss: {train_result_vqa.training_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    trainer_vqa.save_model(\"./vqa_model_final\")\n",
    "    print(\"\\nModel saved to ./vqa_model_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Evaluate VQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if vqa_dataset:\n",
    "    def evaluate_vqa_accuracy(model, processor, dataset, num_samples=100):\n",
    "        \"\"\"Evaluate VQA model accuracy.\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        print(f\"Evaluating on {num_samples} samples...\")\n",
    "        \n",
    "        # TODO: Generate answers and compare with ground truth\n",
    "        for idx in tqdm(range(min(num_samples, len(dataset)))):\n",
    "            sample = dataset[idx]\n",
    "            image = sample['image']\n",
    "            question = sample['question']\n",
    "            true_answer = sample.get('answer', sample.get('answers', ''))\n",
    "            \n",
    "            if isinstance(true_answer, list):\n",
    "                true_answer = true_answer[0]\n",
    "            \n",
    "            # Generate answer\n",
    "            predicted_answer = answer_vqa(image, question, model, processor)\n",
    "            \n",
    "            predictions.append(predicted_answer)\n",
    "            references.append(true_answer)\n",
    "            \n",
    "            # Simple exact match (you can use more sophisticated matching)\n",
    "            if predicted_answer.lower().strip() == true_answer.lower().strip():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': predictions[:10],\n",
    "            'references': references[:10]\n",
    "        }\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\n=== VQA Evaluation ===\")\n",
    "    vqa_results = evaluate_vqa_accuracy(vqa_model, vqa_processor, val_vqa, num_samples=100)\n",
    "    \n",
    "    print(f\"\\nAccuracy: {vqa_results['accuracy']*100:.2f}%\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\n=== Sample Predictions ===\")\n",
    "    for i in range(5):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Reference: {vqa_results['references'][i]}\")\n",
    "        print(f\"Predicted: {vqa_results['predictions'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Analyze Performance by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Categorize questions and analyze performance\n",
    "# Common question types: What, Where, When, Who, How many, Yes/No\n",
    "\n",
    "if vqa_dataset:\n",
    "    def categorize_question(question):\n",
    "        \"\"\"Categorize question by type.\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        if question_lower.startswith('what'):\n",
    "            return 'What'\n",
    "        elif question_lower.startswith('where'):\n",
    "            return 'Where'\n",
    "        elif question_lower.startswith('who'):\n",
    "            return 'Who'\n",
    "        elif question_lower.startswith('how many') or question_lower.startswith('how much'):\n",
    "            return 'Count'\n",
    "        elif any(word in question_lower for word in ['is', 'are', 'does', 'do', 'can']):\n",
    "            return 'Yes/No'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Analyze by question type\n",
    "    question_types = {}\n",
    "    \n",
    "    for i in range(min(100, len(val_vqa))):\n",
    "        sample = val_vqa[i]\n",
    "        question = sample['question']\n",
    "        q_type = categorize_question(question)\n",
    "        \n",
    "        if q_type not in question_types:\n",
    "            question_types[q_type] = []\n",
    "        question_types[q_type].append(i)\n",
    "    \n",
    "    print(\"\\n=== Question Type Distribution ===\")\n",
    "    for q_type, indices in question_types.items():\n",
    "        print(f\"{q_type}: {len(indices)} questions\")\n",
    "    \n",
    "    # TODO: Evaluate performance per question type\n",
    "    print(\"\\n=== Performance by Question Type ===\")\n",
    "    print(\"(Implement detailed analysis here)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Analysis Questions\n",
    "\n",
    "**TODO: Answer the following questions:**\n",
    "\n",
    "1. What was the overall VQA accuracy? How does it compare to baseline?\n",
    "2. Which question types does the model handle well? Which ones are challenging?\n",
    "3. What patterns do you notice in the model's mistakes?\n",
    "4. How could you improve the model's performance further?\n",
    "5. What are the practical applications of this fine-tuned VQA model?\n",
    "6. How does multimodal understanding differ from single-modality tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis:**\n",
    "\n",
    "[Write your analysis here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Report\n",
    "\n",
    "## Summary of Exercises\n",
    "\n",
    "Write a 1-2 page summary covering:\n",
    "\n",
    "1. **Overview**: Brief description of each exercise and objectives\n",
    "2. **Results**: Key metrics and findings from each exercise\n",
    "3. **Comparison**: Compare the three approaches (contrastive learning, captioning, VQA)\n",
    "4. **Challenges**: What difficulties did you encounter? How did you overcome them?\n",
    "5. **Insights**: What did you learn about multimodal learning?\n",
    "6. **Future Work**: How would you extend these experiments?\n",
    "\n",
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Completed Exercise 1 with code, results, and analysis\n",
    "- [ ] Completed Exercise 2 with code, results, and analysis\n",
    "- [ ] Completed Exercise 3 with code, results, and analysis\n",
    "- [ ] Final report (1-2 pages)\n",
    "- [ ] Model checkpoints saved (LoRA weights)\n",
    "- [ ] All visualizations and evaluation metrics included\n",
    "- [ ] Code is well-documented and reproducible\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "1. Export this notebook as both `.ipynb` and `.pdf`\n",
    "2. Include your final report as a separate PDF\n",
    "3. Zip all model checkpoints (LoRA weights only)\n",
    "4. Submit via Canvas/course portal\n",
    "\n",
    "**Good luck with your homework!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
